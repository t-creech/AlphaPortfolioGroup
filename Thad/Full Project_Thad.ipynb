{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import wrds\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "# To this:\n",
    "import torch.optim as optim\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "db = wrds.Connection()\n",
    "\n",
    "class AlphaPortfolioData(Dataset):\n",
    "    def __init__(self, start_year=2014, end_year=2020, final_year=2016, lookback=12, G=2):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.G = G\n",
    "        self.merged, self.final_data = self._load_wrds_data(start_year, end_year, final_year)\n",
    "        self.unique_permnos = sorted(self.final_data['permno'].unique())\n",
    "        self.global_max_assets = len(self.unique_permnos)\n",
    "        self.permno_to_idx = {permno: idx for idx, permno in enumerate(self.unique_permnos)}\n",
    "        self.sequences, self.future_returns, self.masks = self._create_sequences()\n",
    "\n",
    "    def _load_wrds_data(self, start_year, end_year, final_year):\n",
    "\n",
    "        permno_list = []\n",
    "        combined_data = pd.DataFrame()\n",
    "\n",
    "        for year in range(start_year, end_year+1):\n",
    "            \n",
    "            start_date = f'{year}-01-01'\n",
    "            end_date = f'{year}-12-31'\n",
    "            \n",
    "            crsp_query = f\"\"\"\n",
    "                SELECT a.permno, a.date, a.ret, a.prc, a.shrout, \n",
    "                    a.vol, a.cfacshr, a.altprc, a.retx\n",
    "                FROM crsp.msf AS a\n",
    "                WHERE a.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "                AND a.permno IN (\n",
    "                    SELECT permno FROM crsp.msenames \n",
    "                    WHERE exchcd BETWEEN 1 AND 3  \n",
    "                        AND shrcd IN (10, 11)       \n",
    "                    )\n",
    "                \"\"\"\n",
    "            crsp_data = db.raw_sql(crsp_query)\n",
    "\n",
    "            query_ticker = \"\"\"\n",
    "                SELECT permno, namedt, nameenddt, ticker\n",
    "                FROM crsp.stocknames\n",
    "            \"\"\"\n",
    "            \n",
    "            stocknames = db.raw_sql(query_ticker)\n",
    "            crsp_data = crsp_data.merge(stocknames.drop_duplicates(subset=['permno']), on='permno', how='left')\n",
    "            crsp_data = crsp_data.dropna(subset=['ticker'])\n",
    "\n",
    "            crsp_data['mktcap'] = (crsp_data['prc'].abs() * crsp_data['shrout'] * 1000) / 1e6  # In millions\n",
    "            crsp_data['year'] = pd.to_datetime(crsp_data['date']).dt.year\n",
    "            crsp_data = crsp_data.dropna(subset=['mktcap'])\n",
    "            \n",
    "            top_50_permnos_by_year = crsp_data.groupby('permno')['mktcap'].agg(['max']).reset_index().sort_values(by='max', ascending=False).head(50)['permno'].unique()\n",
    "            permno_list.extend(top_50_permnos_by_year)\n",
    "            \n",
    "            combined_data = pd.concat([combined_data, crsp_data[crsp_data['permno'].isin(permno_list)]], axis=0)\n",
    "\n",
    "        combined_data = combined_data[['permno', 'ticker', 'date', 'ret', 'prc', 'shrout', 'vol', 'mktcap', 'year']]\n",
    "        combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "\n",
    "        start_date = f'{start_year}-01-01'\n",
    "        end_date = f'{end_year}-12-31'\n",
    "\n",
    "        # Query Compustat quarterly data with release dates (rdq)\n",
    "        fund_query = f\"\"\"\n",
    "            SELECT gvkey, datadate, rdq, saleq\n",
    "            FROM comp.fundq\n",
    "            WHERE indfmt = 'INDL' AND datafmt = 'STD' AND popsrc = 'D' AND consol = 'C'\n",
    "            AND datadate BETWEEN '{start_date}' AND '{end_date}'\n",
    "            AND rdq IS NOT NULL\n",
    "        \"\"\"\n",
    "        fund = db.raw_sql(fund_query)\n",
    "        fund['rdq'] = pd.to_datetime(fund['rdq'])\n",
    "        fund['datadate'] = pd.to_datetime(fund['datadate'])\n",
    "\n",
    "        # Link Compustat GVKEY to CRSP PERMNO\n",
    "        link_query = \"\"\"\n",
    "            SELECT lpermno AS permno, gvkey, linkdt, linkenddt\n",
    "            FROM crsp.ccmxpf_linktable\n",
    "            WHERE linktype IN ('LU', 'LC') AND linkprim IN ('P', 'C')\n",
    "        \"\"\"\n",
    "        link = db.raw_sql(link_query)\n",
    "        fund = pd.merge(fund, link, on='gvkey', how='left')\n",
    "        fund = fund.dropna(subset=['permno'])\n",
    "\n",
    "        # Sort both datasets by date\n",
    "        combined_data_sorted = combined_data.sort_values('date')\n",
    "        fund_sorted = fund.sort_values('rdq')\n",
    "        fund_sorted['permno'] = fund_sorted['permno'].astype(int)\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            combined_data_sorted,\n",
    "            fund_sorted,\n",
    "            left_on='date',\n",
    "            right_on='rdq',\n",
    "            by='permno',\n",
    "            direction='backward'\n",
    "        )\n",
    "        # merged = merged.dropna(subset=['rdq', 'ticker'])\n",
    "        merged = merged.sort_values(by='date')\n",
    "        merged = merged[['permno', 'ticker', 'date', 'ret', 'prc','vol', 'mktcap', 'gvkey', 'rdq', 'saleq']]\n",
    "        merged = merged.ffill()\n",
    "\n",
    "        unique_dates = merged['date'].unique()\n",
    "        date_mapping = {date: i for i, date in enumerate(sorted(unique_dates))}\n",
    "        merged['date_mapped'] = merged['date'].map(date_mapping)\n",
    "\n",
    "        merged['year'] = pd.to_datetime(merged['date']).dt.year\n",
    "        final_data = merged[merged['year'] >= final_year]\n",
    "\n",
    "        \n",
    "        return merged, final_data\n",
    "\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        data = self.final_data\n",
    "        lookback = self.lookback\n",
    "        unique_dates = pd.to_datetime(data['date'].unique())\n",
    "        unique_dates_sorted = np.sort(unique_dates)\n",
    "        num_features = 6  # Based on []'permno', 'ret', 'prc', 'vol', 'mktcap', 'saleq']\n",
    "\n",
    "        sequences = []\n",
    "        future_returns = []\n",
    "        masks = []\n",
    "\n",
    "        for start_idx in tqdm(range(len(unique_dates_sorted) - 2 * lookback+1)):\n",
    "            hist_start = unique_dates_sorted[start_idx]\n",
    "            hist_end = unique_dates_sorted[start_idx + lookback - 1]\n",
    "            future_start = unique_dates_sorted[start_idx + lookback]\n",
    "            future_end = unique_dates_sorted[start_idx + 2 * lookback-1]\n",
    "\n",
    "            print(f'Hist start: {hist_start}, Hist end: {hist_end}, Future start: {future_start}, Future end: {future_end}')\n",
    "\n",
    "            # Initialize batch arrays with zeros\n",
    "            batch_features = np.zeros((self.global_max_assets, lookback, num_features))\n",
    "            batch_returns = np.zeros((self.global_max_assets, lookback))\n",
    "            batch_mask = np.zeros(self.global_max_assets, dtype=bool)\n",
    "\n",
    "            for permno in self.unique_permnos:\n",
    "                idx = self.permno_to_idx[permno]\n",
    "\n",
    "                # Historical data for the current window\n",
    "                hist_data = data[\n",
    "                    (data['permno'] == permno) &\n",
    "                    (data['date'] >= hist_start) &\n",
    "                    (data['date'] <= hist_end)\n",
    "                ].sort_values('date')\n",
    "\n",
    "                # Future returns for the next window\n",
    "                future_data = data[\n",
    "                    (data['permno'] == permno) &\n",
    "                    (data['date'] >= future_start) &\n",
    "                    (data['date'] <= future_end)\n",
    "                ]['ret'].values\n",
    "\n",
    "                # Check if both periods have complete data\n",
    "                if len(hist_data) == lookback and len(future_data) == lookback:\n",
    "                    features = hist_data[['permno', 'ret', 'prc', 'vol', 'mktcap', 'saleq']].values\n",
    "                    batch_features[idx] = features\n",
    "                    batch_returns[idx] = future_data\n",
    "                    batch_mask[idx] = True\n",
    "\n",
    "            sequences.append(batch_features)\n",
    "            future_returns.append(batch_returns)\n",
    "            masks.append(batch_mask)\n",
    "\n",
    "        # Convert to tensors\n",
    "        sequences_tensor = torch.tensor(np.array(sequences), dtype=torch.float32)\n",
    "        future_returns_tensor = torch.tensor(np.array(future_returns), dtype=torch.float32)\n",
    "        masks_tensor = torch.tensor(np.array(masks), dtype=torch.bool)\n",
    "\n",
    "        return sequences_tensor, future_returns_tensor, masks_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.future_returns[idx], self.masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Positional Encoding Module\n",
    "# ---------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"\n",
    "        Creates sinusoidal positional encodings.\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the embeddings.\n",
    "            dropout (float): Dropout probability.\n",
    "            max_len (int): Maximum expected sequence length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch_size, d_model)\n",
    "        seq_len = x.size(0)\n",
    "        x = x + self.pe[:seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Transformer Encoderâ€“Based AlphaPortfolio Model\n",
    "# -----------------------------------------\n",
    "class TransformerAlphaPortfolio(nn.Module):\n",
    "    def __init__(self, num_features, lookback = 12, d_model=256, nhead=4, num_layers=2, output_dim=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): Number of input features per time step (e.g., 6).\n",
    "            d_model (int): The dimension of the embedding (model dimension).\n",
    "            nhead (int): Number of heads in the multihead attention mechanism.\n",
    "            num_layers (int): Number of Transformer encoder layers.\n",
    "            output_dim (int): Dimension of the model's final output per asset.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 1. Embed the raw features into d_model dimensions.\n",
    "        self.embedding = nn.Linear(num_features, d_model)\n",
    "        \n",
    "        # 2. Positional encoding is applied along the time (lookback) dimension.\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # 3. Transformer encoder layers.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.d_model = d_model\n",
    "        self.fc = nn.Linear(lookback * d_model, output_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, x, asset_mask=None, time_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, global_max_assets, lookback, num_features).\n",
    "            asset_mask (optional): Bool tensor of shape (batch_size, global_max_assets) \n",
    "                                indicating valid assets. (True = valid)\n",
    "            time_mask (optional): Bool tensor of shape (batch_size * global_max_assets, lookback) \n",
    "                                to mask time steps (True = \"should be ignored/padded\").\n",
    "            \n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, global_max_assets, output_dim).\n",
    "        \"\"\"\n",
    "        batch_size, num_assets, lookback, num_features = x.size()\n",
    "        \n",
    "        # 1) Linear embed the raw features: (B, N, L, F) -> (B, N, L, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # 2) Flatten batch & asset for the Transformer:\n",
    "        #    shape -> (B*N, L, d_model)\n",
    "        x = x.view(batch_size * num_assets, lookback, self.d_model)\n",
    "        \n",
    "        # 3) Permute for Transformer input: (L, B*N, d_model)\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # 4) Positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # 5) Pass through Transformer encoder.\n",
    "        #    If you have a time mask, shape should be (B*N, L).\n",
    "        #    True in the mask means \"ignore this time step.\"\n",
    "        encoded_seq = self.transformer_encoder(x, src_key_padding_mask=time_mask)\n",
    "        # encoded_seq shape: (L, B*N, d_model)\n",
    "        \n",
    "        # 6) Concatenate all time-step embeddings:\n",
    "        #    (L, B*N, d_model) -> (B*N, L, d_model) -> (B*N, L*d_model)\n",
    "        encoded_seq = encoded_seq.transpose(0, 1)  # (B*N, L, d_model)\n",
    "        asset_repr = encoded_seq.reshape(batch_size * num_assets, lookback * self.d_model)\n",
    "        \n",
    "        # 7) Final linear layer -> (B*N, output_dim)\n",
    "        #    NOTE: self.fc must be defined as: nn.Linear(lookback * d_model, output_dim)\n",
    "        out = self.fc(asset_repr)\n",
    "        \n",
    "        # 8) Reshape back to (batch_size, num_assets, output_dim)\n",
    "        out = out.view(batch_size, num_assets, -1)\n",
    "        \n",
    "        # 9) Optionally zero out invalid assets\n",
    "        if asset_mask is not None:\n",
    "            # asset_mask: (B, N), True = valid. \n",
    "            # We want to keep outputs only for valid assets, so multiply by float mask.\n",
    "            out = out * asset_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "data = AlphaPortfolioData(start_year=2014, end_year=2020, final_year=2016, lookback=12, G=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = data.sequences\n",
    "future_returns = data.future_returns\n",
    "masks = data.masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([37, 69, 12, 6]), torch.Size([37, 69, 12]), torch.Size([37, 69]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape, future_returns.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ThadCreech/opt/anaconda3/envs/AlphaPortfolio/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "batch_size = sequences.size(0)\n",
    "global_max_assets = data.global_max_assets\n",
    "lookback = data.lookback\n",
    "num_features = sequences.size(-1)\n",
    "output_dim = 1\n",
    "d_model = 256\n",
    "nhead = 4\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "model = TransformerAlphaPortfolio(num_features, lookback, d_model, nhead, num_layers, output_dim, dropout)\n",
    "te_output = model(sequences, asset_mask = masks, time_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6860],\n",
       "         [-1.1090],\n",
       "         [-0.6875],\n",
       "         ...,\n",
       "         [-0.4040],\n",
       "         [-0.7860],\n",
       "         [-0.0000]],\n",
       "\n",
       "        [[-1.1120],\n",
       "         [-1.1664],\n",
       "         [-1.2521],\n",
       "         ...,\n",
       "         [-0.3522],\n",
       "         [-0.6852],\n",
       "         [-0.0000]],\n",
       "\n",
       "        [[-1.1199],\n",
       "         [-0.5493],\n",
       "         [-1.3024],\n",
       "         ...,\n",
       "         [-0.4405],\n",
       "         [-0.5125],\n",
       "         [-0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7586],\n",
       "         [-0.4089],\n",
       "         [-0.0000],\n",
       "         ...,\n",
       "         [-1.1593],\n",
       "         [-0.4148],\n",
       "         [-0.0000]],\n",
       "\n",
       "        [[-0.6823],\n",
       "         [-0.4833],\n",
       "         [ 0.0000],\n",
       "         ...,\n",
       "         [-0.8301],\n",
       "         [-0.7472],\n",
       "         [-0.0000]],\n",
       "\n",
       "        [[-1.4638],\n",
       "         [-0.8607],\n",
       "         [ 0.0000],\n",
       "         ...,\n",
       "         [-0.3784],\n",
       "         [-0.7661],\n",
       "         [ 0.0000]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaPortfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
