{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrds_connection():\n",
    "    \"\"\"Create a WRDS database connection.\"\"\"\n",
    "    db = wrds.Connection()\n",
    "    return db\n",
    "\n",
    "###############################################################################\n",
    "#                             CRSP QUERIES                                    #\n",
    "###############################################################################\n",
    "\n",
    "def get_crsp_monthly_data(db, start_date='2000-01-01', end_date='2005-12-31'):\n",
    "    \"\"\"\n",
    "    Pull monthly CRSP data (msf) within [start_date, end_date].\n",
    "    We include permno, date (month-end), ret, prc, shrout, and compute market cap.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT m.permno,\n",
    "       m.date,\n",
    "       m.ret,\n",
    "       m.prc,\n",
    "       m.shrout,\n",
    "       m.vol,\n",
    "       m.retx\n",
    "    FROM crsp.msf m\n",
    "    JOIN crsp.stocknames n\n",
    "    ON m.permno = n.permno\n",
    "     AND m.date BETWEEN n.namedt AND n.nameenddt\n",
    "    WHERE m.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "     AND n.shrcd IN (10, 11);\n",
    "    \"\"\"\n",
    "    df = db.raw_sql(query, date_cols=['date'])\n",
    "    \n",
    "    # Clean up\n",
    "    df.dropna(subset=['permno','date','prc','shrout'], inplace=True)\n",
    "    df['prc'] = df['prc'].abs()  # CRSP can store negative prices\n",
    "    df['ret'] = pd.to_numeric(df['ret'], errors='coerce')\n",
    "    df['shrout'] = pd.to_numeric(df['shrout'], errors='coerce')\n",
    "    \n",
    "    # Market cap in thousands of dollars (prc * shrout).\n",
    "    # For ranking, the scale doesn’t matter, but you could multiply by 1,000 if you need exact $.\n",
    "    df['mcap'] = df['prc'] * df['shrout']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_top_n_by_year(df_crsp, N=50):\n",
    "    \"\"\"\n",
    "    Process the dataframe in the following steps:\n",
    "      1. Extract the year from the date.\n",
    "      2. For each year and company (permno), compute the maximum market cap.\n",
    "      3. For each year, identify the top N companies and record (if not already recorded) \n",
    "         the year of their first appearance in the top N.\n",
    "      4. Map this \"entry_year\" back onto the original dataframe.\n",
    "      5. Filter the dataframe to include only companies that have ever been in the top N,\n",
    "         but retain all of their data (even data from before their entry year).\n",
    "         \n",
    "    The resulting dataframe contains an extra column, 'entry_year', indicating the first year \n",
    "    the company entered the top N.\n",
    "    \n",
    "    Parameters:\n",
    "      df_crsp (pd.DataFrame): DataFrame containing at least:\n",
    "                              - 'date': dates (datetime or convertible)\n",
    "                              - 'mcap': market capitalization\n",
    "                              - 'permno': unique company identifier.\n",
    "      N (int): Number of top companies to select per year (default 50).\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: Updated DataFrame that includes all data for companies that have ever been in \n",
    "                    the top N, along with a new column 'entry_year' for each company.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure date is in datetime format and extract the year.\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_crsp['date']):\n",
    "        df_crsp['date'] = pd.to_datetime(df_crsp['date'])\n",
    "    df_crsp['year'] = df_crsp['date'].dt.year\n",
    "\n",
    "    # Step 2: For each year and permno, compute the maximum market cap for that year.\n",
    "    df_yearly = df_crsp.groupby(['year', 'permno'], as_index=False)['mcap'].max()\n",
    "    \n",
    "    # Initialize a dictionary to record each permno's first entry year into the top N.\n",
    "    entry_year_dict = {}\n",
    "    \n",
    "    # Process the years in ascending order.\n",
    "    years = sorted(df_yearly['year'].unique())\n",
    "    for yr in years:\n",
    "        # Select data for the current year.\n",
    "        df_current = df_yearly[df_yearly['year'] == yr]\n",
    "        # Sort companies by mcap (descending) and take the top N.\n",
    "        df_current_sorted = df_current.sort_values('mcap', ascending=False)\n",
    "        top_permnos = df_current_sorted.head(N)['permno'].unique()\n",
    "        \n",
    "        # For each company in the top N, if we haven’t yet recorded an entry year, record it.\n",
    "        for permno in top_permnos:\n",
    "            if permno not in entry_year_dict:\n",
    "                entry_year_dict[permno] = yr\n",
    "    \n",
    "    # Step 4: Map the entry year back to the original dataframe.\n",
    "    df_crsp['entry_year'] = df_crsp['permno'].map(entry_year_dict)\n",
    "    \n",
    "    # Step 5: Filter out companies that never made the top N.\n",
    "    df_filtered = df_crsp[df_crsp['entry_year'].notnull()].copy()\n",
    "    \n",
    "    # Optionally drop the temporary 'year' column.\n",
    "    df_filtered.drop(columns=['year'], inplace=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "###############################################################################\n",
    "#                       COMPUSTAT (Quarterly) + LINKING                       #\n",
    "###############################################################################\n",
    "\n",
    "def get_compustat_quarterly_fundamentals(db, start_date='2000-01-01', end_date='2005-12-31'):\n",
    "    \"\"\"\n",
    "    Pull quarterly fundamentals from comp.fundq, restricting to [start_date, end_date]\n",
    "    based on datadate (the quarter-end date).\n",
    "    \n",
    "    We'll define a 'release_date' using rdq if available, else datadate + 45 days.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT gvkey,\n",
    "               datadate,    -- Quarter-end date\n",
    "               rdq,         -- Earnings announcement date\n",
    "               fyearq, fqtr,\n",
    "               atq,         -- Total assets\n",
    "               actq,        -- Current Assets\n",
    "               ceqq,        -- Common/Ordinary Equity - Total\n",
    "               cheq,        -- Cash and Cash Equivs\n",
    "               cogsq,       -- Cost of Goods Sold\n",
    "               dlcq,        -- Debt in Current Liabilities\n",
    "               dlttq,       -- Long-Term Debt - Total\n",
    "               dpq,         -- Depreciation and Amortization - Total\n",
    "               ibq,         -- Income Before Extraordinary Item\n",
    "               invtq,       -- Inventories - Total\n",
    "               lctq,        -- Current Liabilities\n",
    "               ltq,         -- Liabilities Total\n",
    "               niq,         -- Net income\n",
    "               ppentq,      -- Property Plant and Equipment - Total\n",
    "               saleq,       -- Sales/Turnover (Net)\n",
    "               seqq         -- Stockholders Equity\n",
    "               \n",
    "        FROM comp.fundq\n",
    "        WHERE indfmt='INDL'\n",
    "          AND datafmt='STD'\n",
    "          AND popsrc='D'\n",
    "          AND consol='C'\n",
    "          AND datadate BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\"\n",
    "    df_q = db.raw_sql(query, date_cols=['datadate','rdq'])\n",
    "    \n",
    "    # Approximate the release_date\n",
    "    df_q['release_date'] = df_q['rdq']\n",
    "    missing_rdq = df_q['release_date'].isna()\n",
    "    # If rdq is missing, assume 45 days after quarter-end\n",
    "    df_q.loc[missing_rdq, 'release_date'] = df_q.loc[missing_rdq, 'datadate'] + pd.Timedelta(days=45)\n",
    "    \n",
    "    df_q.dropna(subset=['gvkey','datadate','release_date'], inplace=True)\n",
    "    \n",
    "    return df_q\n",
    "\n",
    "def link_ccm(db, df_fundq):\n",
    "    \"\"\"\n",
    "    Link quarterly fundamentals (gvkey) to CRSP (permno) via the CCM link table,\n",
    "    keeping only valid link types and ensuring datadate is within link date range.\n",
    "    \"\"\"\n",
    "    ccm_query = \"\"\"\n",
    "        SELECT gvkey,\n",
    "               lpermno AS permno,\n",
    "               linkdt,\n",
    "               linkenddt\n",
    "        FROM crsp.ccmxpf_linktable\n",
    "        WHERE linktype IN ('LC','LU','LX','LD','LS','LN')\n",
    "          AND linkprim IN ('P','C','J')\n",
    "    \"\"\"\n",
    "    df_link = db.raw_sql(ccm_query, date_cols=['linkdt','linkenddt'])\n",
    "    df_link['linkenddt'] = df_link['linkenddt'].fillna(pd.to_datetime('2099-12-31'))\n",
    "    \n",
    "    # Merge fundamentals with link table\n",
    "    df_merged = pd.merge(df_fundq, df_link, on='gvkey', how='left')\n",
    "    \n",
    "    # Keep rows where datadate is within link date range\n",
    "    mask = (df_merged['datadate'] >= df_merged['linkdt']) & \\\n",
    "           (df_merged['datadate'] <= df_merged['linkenddt'])\n",
    "    df_merged = df_merged[mask].copy()\n",
    "    df_merged['permno'] = df_merged['permno'].astype('int64')\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "###############################################################################\n",
    "#                    CARRY-FORWARD MERGE (MONTHLY CRSP)                       #\n",
    "###############################################################################\n",
    "\n",
    "def carry_forward_monthly_fundamentals(df_crsp_m, df_ccm_q):\n",
    "    \"\"\"\n",
    "    Attach fundamentals to monthly CRSP by carrying forward the \n",
    "    most recent statement (release_date <= CRSP date).\n",
    "    \n",
    "    Steps:\n",
    "      1. Sort CRSP data by (permno, date).\n",
    "      2. Sort fundamentals by (permno, release_date).\n",
    "      3. merge_asof(direction='backward', by='permno')\n",
    "      4. Drop rows with no matched fundamentals (if desired).\n",
    "    \"\"\"\n",
    "    df_crsp_m.sort_values(by=['date','permno'], ascending=[True, True], inplace=True)\n",
    "    df_ccm_q.sort_values(by=['release_date','permno'], ascending=[True, True], inplace=True)\n",
    "    \n",
    "    merged = pd.merge_asof(\n",
    "        left=df_crsp_m,\n",
    "        right=df_ccm_q,\n",
    "        left_on='date',\n",
    "        right_on='release_date',\n",
    "        by='permno',\n",
    "        direction='backward'\n",
    "    )\n",
    "    \n",
    "    # Remove rows with no fundamentals (no prior statement)\n",
    "    merged.dropna(subset=['gvkey'], inplace=True)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "###############################################################################\n",
    "#                              CLEAN DATA                                     #\n",
    "###############################################################################\n",
    "\n",
    "def filter_data(df, n):\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame that contains monthly data for companies (identified by 'permno')\n",
    "    in two steps:\n",
    "    \n",
    "    1. Month Filtering:\n",
    "       Only keep months (year-month periods) where the number of unique companies exceeds n.\n",
    "    \n",
    "    2. Recursive Month-to-Month Validation (Within Each Year):\n",
    "       For each year, start with the companies in the first month.\n",
    "       Then for each subsequent month, only include a company's data if that company was present in\n",
    "       the previous month. This check is applied month-by-month until reaching a month where there is\n",
    "       no earlier month in that year (the first month is always accepted).\n",
    "       \n",
    "       In effect, a company’s data for a given month is included only if it has a continuous presence \n",
    "       (from some start month of that year) up to that month.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame containing at least:\n",
    "                         - 'date': monthly dates (or strings convertible to datetime)\n",
    "                         - 'permno': unique company identifier.\n",
    "      n (int): Minimum number of unique companies that must be present in a month for it to be kept.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # --- Preparation: Ensure dates and add helper columns ---\n",
    "    df = df.copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Create a 'year' column and a 'year_month' Period column (e.g., \"2020-01\").\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['year_month'] = df['date'].dt.to_period('M')\n",
    "    \n",
    "    # --- Step 1: Month Filtering ---\n",
    "    # Count unique companies per month.\n",
    "    month_counts = df.groupby('year_month')['permno'].nunique()\n",
    "    # Identify valid months where the unique count is greater than n.\n",
    "    valid_months = month_counts[month_counts >= n/(1.25)].index\n",
    "    df_filtered = df[df['year_month'].isin(valid_months)].copy()\n",
    "    \n",
    "    # --- Step 2: Recursive Month-to-Month Validation ---\n",
    "    # We will process each year separately.\n",
    "    cleaned_years = []  # list to hold cleaned data for each year.\n",
    "    \n",
    "    for yr, group in df_filtered.groupby('year'):\n",
    "        # Sort the group by date to ensure months are in order.\n",
    "        group = group.sort_values('date').copy()\n",
    "        # Get the unique months for the year in order.\n",
    "        months = sorted(group['year_month'].unique())\n",
    "        \n",
    "        # This dictionary will map each month to the set of companies that \"pass\" the chain for that month.\n",
    "        valid_by_month = {}\n",
    "        \n",
    "        for i, m in enumerate(months):\n",
    "            # Get the set of companies in month m.\n",
    "            companies_current = set(group[group['year_month'] == m]['permno'].unique())\n",
    "            \n",
    "            if i == 0:\n",
    "                # First month of the year: accept all companies.\n",
    "                valid_by_month[m] = companies_current\n",
    "            else:\n",
    "                # For subsequent months, only keep companies that were valid in the immediately previous month.\n",
    "                prev_month = months[i-1]\n",
    "                valid_by_month[m] = companies_current.intersection(valid_by_month[prev_month])\n",
    "        \n",
    "        # Now, for each month in this year, filter rows to only those companies that passed the chain.\n",
    "        frames = []\n",
    "        for m in months:\n",
    "            valid_companies = valid_by_month[m]\n",
    "            # Filter the data for month m to only include rows for valid companies.\n",
    "            frames.append(group[(group['year_month'] == m) & (group['permno'].isin(valid_companies))])\n",
    "        \n",
    "        if frames:\n",
    "            cleaned_years.append(pd.concat(frames))\n",
    "    \n",
    "    # Combine all years back together.\n",
    "    df_cleaned = pd.concat(cleaned_years, ignore_index=True)\n",
    "    \n",
    "    # Optionally, drop helper columns.\n",
    "    df_cleaned.drop(columns=['year', 'year_month'], inplace=True)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def impute_fundamentals(df):\n",
    "    \"\"\"Cleans & imputes missing values for key financial variables\"\"\"\n",
    "    \n",
    "    # Ensure the DataFrame is sorted by company and date\n",
    "    df.sort_values(by=[\"permno\", \"date\"], inplace=True)\n",
    "    \n",
    "    # Convert 0s to NaN for selected variables (verify that 0 is not a valid value here)\n",
    "    zero_to_nan_cols = [\"dpq\", \"saleq\", \"cogsq\"]\n",
    "    df[zero_to_nan_cols] = df[zero_to_nan_cols].replace(0, np.nan)\n",
    "    \n",
    "    # Forward and backward fill for balance sheet items (these items tend to change slowly)\n",
    "    balance_sheet_cols = [\"atq\", \"actq\", \"ceqq\", \"cheq\", \"cogsq\", \"dlcq\", \"dlttq\", \"dpq\", \"invtq\", \"lctq\", \"ltq\", \"ppentq\", \"seqq\"]\n",
    "    df[balance_sheet_cols] = df.groupby(\"permno\")[balance_sheet_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    \n",
    "    # Rolling mean imputation for trend-based variables (e.g., earnings measures)\n",
    "    trend_cols = [\"ibq\", \"niq\", \"saleq\"]\n",
    "    for col in trend_cols:\n",
    "        df[col] = df.groupby(\"permno\")[col].transform(lambda x: x.fillna(x.rolling(4, min_periods=1).mean()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the input DataFrame by:\n",
    "      1. Handling missing values using forward-fill, backward-fill, and rolling mean.\n",
    "      2. Converting date columns to datetime format.\n",
    "      3. Removing duplicate rows.\n",
    "      4. Fixing data types for categorical and numerical columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Handle Missing Values & Impute Fundamentals\n",
    "    df = impute_fundamentals(df)\n",
    "    \n",
    "    # 2. Drop unnecessary columns\n",
    "    drop_cols = [\"gvkey\", \"datadate\", \"rdq\", \"fyearq\", \"fqtr\", \"release_date\", \"linkdt\", \"linkenddt\"]\n",
    "    for col in drop_cols:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # 2. Convert Date Columns to Datetime\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    # 3. Remove Duplicate Entries\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    Rename columns to more descriptive names.\n",
    "    \"\"\"\n",
    "    new_columns = {\n",
    "        \"atq\": \"total_assets\",\n",
    "        \"actq\": \"current_assets\",\n",
    "        \"ceqq\": \"common_equity\",\n",
    "        \"cheq\": \"cash_equivalents\",\n",
    "        \"cogsq\": \"cost_of_goods_sold\",\n",
    "        \"dlcq\": \"debt_current\",\n",
    "        \"dlttq\": \"debt_long_term\",\n",
    "        \"dpq\": \"depreciation_amortization\",\n",
    "        \"ibq\": \"income_before_extraordinary_items\",\n",
    "        \"invtq\": \"inventories\",\n",
    "        \"lctq\": \"liabilities_current\",\n",
    "        \"ltq\": \"liabilities_total\",\n",
    "        \"mcap\": \"market_cap\",\n",
    "        \"niq\": \"net_income\",\n",
    "        \"ppentq\": \"property_plant_equipment\",\n",
    "        \"prc\": \"price\",\n",
    "        \"ret\": \"return\",\n",
    "        \"retx\": \"return_ex_dividends\",\n",
    "        \"saleq\": \"sales\",\n",
    "        \"seqq\": \"stockholders_equity\",\n",
    "        \"shrout\": \"shares_outstanding\",\n",
    "        \"vol\": \"volume\"\n",
    "    }\n",
    "    \n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n",
      "Initial CRSP monthly shape: (489969, 8)\n",
      "CRSP after filtering to top 50 by market cap, shape: (10548, 9)\n",
      "Quarterly fundamentals shape: (507282, 22)\n",
      "After CCM linking shape: (308636, 25)\n",
      "Merged shape: (10310, 33)\n",
      "Filtered shape: (10149, 33)\n",
      "Final shape: (10149, 25)\n",
      "\n",
      "Data pipeline complete! Saved to monthly_top50_same_range_carried_forward.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/flcpjk4s0bqdtnhxz0py83c80000gn/T/ipykernel_1185/3155818210.py:318: FutureWarning: DataFrameGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use DataFrame.fillna instead\n",
      "  df[balance_sheet_cols] = df.groupby(\"permno\")[balance_sheet_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "/var/folders/y0/flcpjk4s0bqdtnhxz0py83c80000gn/T/ipykernel_1185/3155818210.py:318: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[balance_sheet_cols] = df.groupby(\"permno\")[balance_sheet_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "# Define a single date range for BOTH CRSP and Compustat\n",
    "START_DATE = '2010-01-01'\n",
    "END_DATE   = '2020-12-31'\n",
    "TOP_N      = 50\n",
    "\n",
    "db = wrds_connection()\n",
    "\n",
    "# 1. Pull monthly CRSP in [start_date, end_date]\n",
    "crsp_m = get_crsp_monthly_data(db, start_date=START_DATE, end_date=END_DATE)\n",
    "print(\"Initial CRSP monthly shape:\", crsp_m.shape)\n",
    "\n",
    "# 2. Filter to top N by market cap, across the entire timeframe\n",
    "crsp_m_topN = filter_top_n_by_year(crsp_m, N=TOP_N)\n",
    "print(f\"CRSP after filtering to top {TOP_N} by market cap, shape:\", crsp_m_topN.shape)\n",
    "\n",
    "# 3. Pull quarterly fundamentals from Compustat, restricted to same [start_date, end_date]\n",
    "comp_q = get_compustat_quarterly_fundamentals(db, start_date=START_DATE, end_date=END_DATE)\n",
    "print(\"Quarterly fundamentals shape:\", comp_q.shape)\n",
    "\n",
    "# 4. Link to CRSP via CCM\n",
    "comp_q_ccm = link_ccm(db, comp_q)\n",
    "print(\"After CCM linking shape:\", comp_q_ccm.shape)\n",
    "\n",
    "# 5. Carry-forward fundamentals for monthly CRSP\n",
    "merged_df = carry_forward_monthly_fundamentals(crsp_m_topN, comp_q_ccm)\n",
    "print(\"Merged shape:\", merged_df.shape)\n",
    "\n",
    "# 6. Filter data to remove odd companies who do not have data for the entire year\n",
    "filtered_df = filter_data(merged_df, n=TOP_N)\n",
    "print(\"Filtered shape:\", filtered_df.shape)\n",
    "\n",
    "# 7. Clean data to handle missing values, etc.\n",
    "cleansed_df = clean_data(filtered_df)\n",
    "final_df = rename_columns(cleansed_df)\n",
    "print(\"Final shape:\", final_df.shape)\n",
    "\n",
    "# Example look at a few columns\n",
    "final_df.head(10)\n",
    "\n",
    "# Save result\n",
    "final_df.to_csv(\"monthly_top50_same_range_carried_forward.csv\", index=False)\n",
    "print(\"\\nData pipeline complete! Saved to monthly_top50_same_range_carried_forward.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaPortfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
