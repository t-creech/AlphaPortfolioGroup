{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import wrds\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "# To this:\n",
    "import torch.optim as optim\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "db = wrds.Connection()\n",
    "\n",
    "class AlphaPortfolioData(Dataset):\n",
    "    def __init__(self, start_year=2014, end_year=2020, final_year=2016, lookback=12, G=2):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.G = G\n",
    "        self.merged, self.final_data = self._load_wrds_data(start_year, end_year, final_year)\n",
    "        self.unique_permnos = sorted(self.final_data['permno'].unique())\n",
    "        self.global_max_assets = len(self.unique_permnos)\n",
    "        self.permno_to_idx = {permno: idx for idx, permno in enumerate(self.unique_permnos)}\n",
    "        self.sequences, self.future_returns, self.masks = self._create_sequences()\n",
    "\n",
    "    def _load_wrds_data(self, start_year, end_year, final_year):\n",
    "\n",
    "        permno_list = []\n",
    "        combined_data = pd.DataFrame()\n",
    "\n",
    "        for year in range(start_year, end_year+1):\n",
    "            \n",
    "            start_date = f'{year}-01-01'\n",
    "            end_date = f'{year}-12-31'\n",
    "            \n",
    "            crsp_query = f\"\"\"\n",
    "                SELECT a.permno, a.date, a.ret, a.prc, a.shrout, \n",
    "                    a.vol, a.cfacshr, a.altprc, a.retx\n",
    "                FROM crsp.msf AS a\n",
    "                WHERE a.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "                AND a.permno IN (\n",
    "                    SELECT permno FROM crsp.msenames \n",
    "                    WHERE exchcd BETWEEN 1 AND 3  \n",
    "                        AND shrcd IN (10, 11)       \n",
    "                    )\n",
    "                \"\"\"\n",
    "            crsp_data = db.raw_sql(crsp_query)\n",
    "\n",
    "            query_ticker = \"\"\"\n",
    "                SELECT permno, namedt, nameenddt, ticker\n",
    "                FROM crsp.stocknames\n",
    "            \"\"\"\n",
    "            \n",
    "            stocknames = db.raw_sql(query_ticker)\n",
    "            crsp_data = crsp_data.merge(stocknames.drop_duplicates(subset=['permno']), on='permno', how='left')\n",
    "            crsp_data = crsp_data.dropna(subset=['ticker'])\n",
    "\n",
    "            crsp_data['mktcap'] = (crsp_data['prc'].abs() * crsp_data['shrout'] * 1000) / 1e6  # In millions\n",
    "            crsp_data['year'] = pd.to_datetime(crsp_data['date']).dt.year\n",
    "            crsp_data = crsp_data.dropna(subset=['mktcap'])\n",
    "            \n",
    "            top_50_permnos_by_year = crsp_data.groupby('permno')['mktcap'].agg(['max']).reset_index().sort_values(by='max', ascending=False).head(50)['permno'].unique()\n",
    "            permno_list.extend(top_50_permnos_by_year)\n",
    "            \n",
    "            combined_data = pd.concat([combined_data, crsp_data[crsp_data['permno'].isin(permno_list)]], axis=0)\n",
    "\n",
    "        combined_data = combined_data[['permno', 'ticker', 'date', 'ret', 'prc', 'shrout', 'vol', 'mktcap', 'year']]\n",
    "        combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "\n",
    "        start_date = f'{start_year}-01-01'\n",
    "        end_date = f'{end_year}-12-31'\n",
    "\n",
    "        # Query Compustat quarterly data with release dates (rdq)\n",
    "        fund_query = f\"\"\"\n",
    "            SELECT gvkey, datadate, rdq, saleq\n",
    "            FROM comp.fundq\n",
    "            WHERE indfmt = 'INDL' AND datafmt = 'STD' AND popsrc = 'D' AND consol = 'C'\n",
    "            AND datadate BETWEEN '{start_date}' AND '{end_date}'\n",
    "            AND rdq IS NOT NULL\n",
    "        \"\"\"\n",
    "        fund = db.raw_sql(fund_query)\n",
    "        fund['rdq'] = pd.to_datetime(fund['rdq'])\n",
    "        fund['datadate'] = pd.to_datetime(fund['datadate'])\n",
    "\n",
    "        # Link Compustat GVKEY to CRSP PERMNO\n",
    "        link_query = \"\"\"\n",
    "            SELECT lpermno AS permno, gvkey, linkdt, linkenddt\n",
    "            FROM crsp.ccmxpf_linktable\n",
    "            WHERE linktype IN ('LU', 'LC') AND linkprim IN ('P', 'C')\n",
    "        \"\"\"\n",
    "        link = db.raw_sql(link_query)\n",
    "        fund = pd.merge(fund, link, on='gvkey', how='left')\n",
    "        fund = fund.dropna(subset=['permno'])\n",
    "\n",
    "        # Sort both datasets by date\n",
    "        combined_data_sorted = combined_data.sort_values('date')\n",
    "        fund_sorted = fund.sort_values('rdq')\n",
    "        fund_sorted['permno'] = fund_sorted['permno'].astype(int)\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            combined_data_sorted,\n",
    "            fund_sorted,\n",
    "            left_on='date',\n",
    "            right_on='rdq',\n",
    "            by='permno',\n",
    "            direction='backward'\n",
    "        )\n",
    "        # merged = merged.dropna(subset=['rdq', 'ticker'])\n",
    "        merged = merged.sort_values(by='date')\n",
    "        merged = merged[['permno', 'ticker', 'date', 'ret', 'prc','vol', 'mktcap', 'gvkey', 'rdq', 'saleq']]\n",
    "        merged = merged.ffill()\n",
    "\n",
    "        unique_dates = merged['date'].unique()\n",
    "        date_mapping = {date: i for i, date in enumerate(sorted(unique_dates))}\n",
    "        merged['date_mapped'] = merged['date'].map(date_mapping)\n",
    "\n",
    "        merged['year'] = pd.to_datetime(merged['date']).dt.year\n",
    "        final_data = merged[merged['year'] >= final_year]\n",
    "\n",
    "        \n",
    "        return merged, final_data\n",
    "\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        data = self.final_data\n",
    "        lookback = self.lookback\n",
    "        unique_dates = pd.to_datetime(data['date'].unique())\n",
    "        unique_dates_sorted = np.sort(unique_dates)\n",
    "        num_features = 6  # Based on []'permno', 'ret', 'prc', 'vol', 'mktcap', 'saleq']\n",
    "\n",
    "        sequences = []\n",
    "        future_returns = []\n",
    "        masks = []\n",
    "\n",
    "        for start_idx in tqdm(range(len(unique_dates_sorted) - 2 * lookback+1)):\n",
    "            hist_start = unique_dates_sorted[start_idx]\n",
    "            hist_end = unique_dates_sorted[start_idx + lookback - 1]\n",
    "            future_start = unique_dates_sorted[start_idx + lookback]\n",
    "            future_end = unique_dates_sorted[start_idx + 2 * lookback-1]\n",
    "\n",
    "            print(f'Hist start: {hist_start}, Hist end: {hist_end}, Future start: {future_start}, Future end: {future_end}')\n",
    "\n",
    "            # Initialize batch arrays with zeros\n",
    "            batch_features = np.zeros((self.global_max_assets, lookback, num_features))\n",
    "            batch_returns = np.zeros((self.global_max_assets, lookback))\n",
    "            batch_mask = np.zeros(self.global_max_assets, dtype=bool)\n",
    "\n",
    "            for permno in self.unique_permnos:\n",
    "                idx = self.permno_to_idx[permno]\n",
    "\n",
    "                # Historical data for the current window\n",
    "                hist_data = data[\n",
    "                    (data['permno'] == permno) &\n",
    "                    (data['date'] >= hist_start) &\n",
    "                    (data['date'] <= hist_end)\n",
    "                ].sort_values('date')\n",
    "\n",
    "                # Future returns for the next window\n",
    "                future_data = data[\n",
    "                    (data['permno'] == permno) &\n",
    "                    (data['date'] >= future_start) &\n",
    "                    (data['date'] <= future_end)\n",
    "                ]['ret'].values\n",
    "\n",
    "                # Check if both periods have complete data\n",
    "                if len(hist_data) == lookback and len(future_data) == lookback:\n",
    "                    features = hist_data[['permno', 'ret', 'prc', 'vol', 'mktcap', 'saleq']].values\n",
    "                    batch_features[idx] = features\n",
    "                    batch_returns[idx] = future_data\n",
    "                    batch_mask[idx] = True\n",
    "\n",
    "            sequences.append(batch_features)\n",
    "            future_returns.append(batch_returns)\n",
    "            masks.append(batch_mask)\n",
    "\n",
    "        # Convert to tensors\n",
    "        sequences_tensor = torch.tensor(np.array(sequences), dtype=torch.float32)\n",
    "        future_returns_tensor = torch.tensor(np.array(future_returns), dtype=torch.float32)\n",
    "        masks_tensor = torch.tensor(np.array(masks), dtype=torch.bool)\n",
    "\n",
    "        return sequences_tensor, future_returns_tensor, masks_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.future_returns[idx], self.masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/37 [00:00<00:02, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2016-01-29T00:00:00.000000000, Hist end: 2016-12-30T00:00:00.000000000, Future start: 2017-01-31T00:00:00.000000000, Future end: 2017-12-29T00:00:00.000000000\n",
      "Hist start: 2016-02-29T00:00:00.000000000, Hist end: 2017-01-31T00:00:00.000000000, Future start: 2017-02-28T00:00:00.000000000, Future end: 2018-01-31T00:00:00.000000000\n",
      "Hist start: 2016-03-31T00:00:00.000000000, Hist end: 2017-02-28T00:00:00.000000000, Future start: 2017-03-31T00:00:00.000000000, Future end: 2018-02-28T00:00:00.000000000\n",
      "Hist start: 2016-04-29T00:00:00.000000000, Hist end: 2017-03-31T00:00:00.000000000, Future start: 2017-04-28T00:00:00.000000000, Future end: 2018-03-29T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 6/37 [00:00<00:01, 16.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2016-05-31T00:00:00.000000000, Hist end: 2017-04-28T00:00:00.000000000, Future start: 2017-05-31T00:00:00.000000000, Future end: 2018-04-30T00:00:00.000000000\n",
      "Hist start: 2016-06-30T00:00:00.000000000, Hist end: 2017-05-31T00:00:00.000000000, Future start: 2017-06-30T00:00:00.000000000, Future end: 2018-05-31T00:00:00.000000000\n",
      "Hist start: 2016-07-29T00:00:00.000000000, Hist end: 2017-06-30T00:00:00.000000000, Future start: 2017-07-31T00:00:00.000000000, Future end: 2018-06-29T00:00:00.000000000\n",
      "Hist start: 2016-08-31T00:00:00.000000000, Hist end: 2017-07-31T00:00:00.000000000, Future start: 2017-08-31T00:00:00.000000000, Future end: 2018-07-31T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 10/37 [00:00<00:01, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2016-09-30T00:00:00.000000000, Hist end: 2017-08-31T00:00:00.000000000, Future start: 2017-09-29T00:00:00.000000000, Future end: 2018-08-31T00:00:00.000000000\n",
      "Hist start: 2016-10-31T00:00:00.000000000, Hist end: 2017-09-29T00:00:00.000000000, Future start: 2017-10-31T00:00:00.000000000, Future end: 2018-09-28T00:00:00.000000000\n",
      "Hist start: 2016-11-30T00:00:00.000000000, Hist end: 2017-10-31T00:00:00.000000000, Future start: 2017-11-30T00:00:00.000000000, Future end: 2018-10-31T00:00:00.000000000\n",
      "Hist start: 2016-12-30T00:00:00.000000000, Hist end: 2017-11-30T00:00:00.000000000, Future start: 2017-12-29T00:00:00.000000000, Future end: 2018-11-30T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 14/37 [00:00<00:01, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2017-01-31T00:00:00.000000000, Hist end: 2017-12-29T00:00:00.000000000, Future start: 2018-01-31T00:00:00.000000000, Future end: 2018-12-31T00:00:00.000000000\n",
      "Hist start: 2017-02-28T00:00:00.000000000, Hist end: 2018-01-31T00:00:00.000000000, Future start: 2018-02-28T00:00:00.000000000, Future end: 2019-01-31T00:00:00.000000000\n",
      "Hist start: 2017-03-31T00:00:00.000000000, Hist end: 2018-02-28T00:00:00.000000000, Future start: 2018-03-29T00:00:00.000000000, Future end: 2019-02-28T00:00:00.000000000\n",
      "Hist start: 2017-04-28T00:00:00.000000000, Hist end: 2018-03-29T00:00:00.000000000, Future start: 2018-04-30T00:00:00.000000000, Future end: 2019-03-29T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 18/37 [00:01<00:01, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2017-05-31T00:00:00.000000000, Hist end: 2018-04-30T00:00:00.000000000, Future start: 2018-05-31T00:00:00.000000000, Future end: 2019-04-30T00:00:00.000000000\n",
      "Hist start: 2017-06-30T00:00:00.000000000, Hist end: 2018-05-31T00:00:00.000000000, Future start: 2018-06-29T00:00:00.000000000, Future end: 2019-05-31T00:00:00.000000000\n",
      "Hist start: 2017-07-31T00:00:00.000000000, Hist end: 2018-06-29T00:00:00.000000000, Future start: 2018-07-31T00:00:00.000000000, Future end: 2019-06-28T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 22/37 [00:01<00:00, 16.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2017-08-31T00:00:00.000000000, Hist end: 2018-07-31T00:00:00.000000000, Future start: 2018-08-31T00:00:00.000000000, Future end: 2019-07-31T00:00:00.000000000\n",
      "Hist start: 2017-09-29T00:00:00.000000000, Hist end: 2018-08-31T00:00:00.000000000, Future start: 2018-09-28T00:00:00.000000000, Future end: 2019-08-30T00:00:00.000000000\n",
      "Hist start: 2017-10-31T00:00:00.000000000, Hist end: 2018-09-28T00:00:00.000000000, Future start: 2018-10-31T00:00:00.000000000, Future end: 2019-09-30T00:00:00.000000000\n",
      "Hist start: 2017-11-30T00:00:00.000000000, Hist end: 2018-10-31T00:00:00.000000000, Future start: 2018-11-30T00:00:00.000000000, Future end: 2019-10-31T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 26/37 [00:01<00:00, 16.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2017-12-29T00:00:00.000000000, Hist end: 2018-11-30T00:00:00.000000000, Future start: 2018-12-31T00:00:00.000000000, Future end: 2019-11-29T00:00:00.000000000\n",
      "Hist start: 2018-01-31T00:00:00.000000000, Hist end: 2018-12-31T00:00:00.000000000, Future start: 2019-01-31T00:00:00.000000000, Future end: 2019-12-31T00:00:00.000000000\n",
      "Hist start: 2018-02-28T00:00:00.000000000, Hist end: 2019-01-31T00:00:00.000000000, Future start: 2019-02-28T00:00:00.000000000, Future end: 2020-01-31T00:00:00.000000000\n",
      "Hist start: 2018-03-29T00:00:00.000000000, Hist end: 2019-02-28T00:00:00.000000000, Future start: 2019-03-29T00:00:00.000000000, Future end: 2020-02-28T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 30/37 [00:01<00:00, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2018-04-30T00:00:00.000000000, Hist end: 2019-03-29T00:00:00.000000000, Future start: 2019-04-30T00:00:00.000000000, Future end: 2020-03-31T00:00:00.000000000\n",
      "Hist start: 2018-05-31T00:00:00.000000000, Hist end: 2019-04-30T00:00:00.000000000, Future start: 2019-05-31T00:00:00.000000000, Future end: 2020-04-30T00:00:00.000000000\n",
      "Hist start: 2018-06-29T00:00:00.000000000, Hist end: 2019-05-31T00:00:00.000000000, Future start: 2019-06-28T00:00:00.000000000, Future end: 2020-05-29T00:00:00.000000000\n",
      "Hist start: 2018-07-31T00:00:00.000000000, Hist end: 2019-06-28T00:00:00.000000000, Future start: 2019-07-31T00:00:00.000000000, Future end: 2020-06-30T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 34/37 [00:02<00:00, 16.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2018-08-31T00:00:00.000000000, Hist end: 2019-07-31T00:00:00.000000000, Future start: 2019-08-30T00:00:00.000000000, Future end: 2020-07-31T00:00:00.000000000\n",
      "Hist start: 2018-09-28T00:00:00.000000000, Hist end: 2019-08-30T00:00:00.000000000, Future start: 2019-09-30T00:00:00.000000000, Future end: 2020-08-31T00:00:00.000000000\n",
      "Hist start: 2018-10-31T00:00:00.000000000, Hist end: 2019-09-30T00:00:00.000000000, Future start: 2019-10-31T00:00:00.000000000, Future end: 2020-09-30T00:00:00.000000000\n",
      "Hist start: 2018-11-30T00:00:00.000000000, Hist end: 2019-10-31T00:00:00.000000000, Future start: 2019-11-29T00:00:00.000000000, Future end: 2020-10-30T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:02<00:00, 16.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2018-12-31T00:00:00.000000000, Hist end: 2019-11-29T00:00:00.000000000, Future start: 2019-12-31T00:00:00.000000000, Future end: 2020-11-30T00:00:00.000000000\n",
      "Hist start: 2019-01-31T00:00:00.000000000, Hist end: 2019-12-31T00:00:00.000000000, Future start: 2020-01-31T00:00:00.000000000, Future end: 2020-12-31T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "data = AlphaPortfolioData(start_year=2014, end_year=2020, final_year=2016, lookback=12, G=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.final_data.to_csv('final_data_Dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaPortfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
