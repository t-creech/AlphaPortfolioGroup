{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "Created .pgpass file successfully.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import wrds\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "# To this:\n",
    "import torch.optim as optim\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "db = wrds.Connection()\n",
    "\n",
    "class AlphaPortfolioData(Dataset):\n",
    "    def __init__(self, start_year=2014, end_year=2020, final_year=2016, lookback=12, G=2):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.G = G\n",
    "        self.merged, self.final_data = self._load_wrds_data(start_year, end_year, final_year)\n",
    "        self.unique_permnos = sorted(self.final_data['permno'].unique())\n",
    "        self.global_max_assets = len(self.unique_permnos)\n",
    "        self.permno_to_idx = {permno: idx for idx, permno in enumerate(self.unique_permnos)}\n",
    "        self.sequences, self.future_returns, self.masks = self._create_sequences()\n",
    "\n",
    "    def _load_wrds_data(self, start_year, end_year, final_year):\n",
    "\n",
    "        permno_list = []\n",
    "        combined_data = pd.DataFrame()\n",
    "\n",
    "        for year in range(start_year, end_year+1):\n",
    "            \n",
    "            start_date = f'{year}-01-01'\n",
    "            end_date = f'{year}-12-31'\n",
    "            \n",
    "            crsp_query = f\"\"\"\n",
    "                SELECT a.permno, a.date, a.ret, a.prc, a.shrout, \n",
    "                    a.vol, a.cfacshr, a.altprc, a.retx\n",
    "                FROM crsp.msf AS a\n",
    "                WHERE a.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "                AND a.permno IN (\n",
    "                    SELECT permno FROM crsp.msenames \n",
    "                    WHERE exchcd BETWEEN 1 AND 3  \n",
    "                        AND shrcd IN (10, 11)       \n",
    "                    )\n",
    "                \"\"\"\n",
    "            crsp_data = db.raw_sql(crsp_query)\n",
    "\n",
    "            query_ticker = \"\"\"\n",
    "                SELECT permno, namedt, nameenddt, ticker\n",
    "                FROM crsp.stocknames\n",
    "            \"\"\"\n",
    "            \n",
    "            stocknames = db.raw_sql(query_ticker)\n",
    "            crsp_data = crsp_data.merge(stocknames.drop_duplicates(subset=['permno']), on='permno', how='left')\n",
    "            crsp_data = crsp_data.dropna(subset=['ticker'])\n",
    "\n",
    "            crsp_data['mktcap'] = (crsp_data['prc'].abs() * crsp_data['shrout'] * 1000) / 1e6  # In millions\n",
    "            crsp_data['year'] = pd.to_datetime(crsp_data['date']).dt.year\n",
    "            crsp_data = crsp_data.dropna(subset=['mktcap'])\n",
    "            \n",
    "            top_50_permnos_by_year = crsp_data.groupby('permno')['mktcap'].agg(['max']).reset_index().sort_values(by='max', ascending=False).head(50)['permno'].unique()\n",
    "            permno_list.extend(top_50_permnos_by_year)\n",
    "            \n",
    "            combined_data = pd.concat([combined_data, crsp_data[crsp_data['permno'].isin(permno_list)]], axis=0)\n",
    "\n",
    "        combined_data = combined_data[['permno', 'ticker', 'date', 'ret', 'prc', 'shrout', 'vol', 'mktcap', 'year']]\n",
    "        combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "\n",
    "        start_date = f'{start_year}-01-01'\n",
    "        end_date = f'{end_year}-12-31'\n",
    "\n",
    "        # Query Compustat quarterly data with release dates (rdq)\n",
    "        fund_query = f\"\"\"\n",
    "            SELECT gvkey, datadate, rdq, saleq\n",
    "            FROM comp.fundq\n",
    "            WHERE indfmt = 'INDL' AND datafmt = 'STD' AND popsrc = 'D' AND consol = 'C'\n",
    "            AND datadate BETWEEN '{start_date}' AND '{end_date}'\n",
    "            AND rdq IS NOT NULL\n",
    "        \"\"\"\n",
    "        fund = db.raw_sql(fund_query)\n",
    "        fund['rdq'] = pd.to_datetime(fund['rdq'])\n",
    "        fund['datadate'] = pd.to_datetime(fund['datadate'])\n",
    "\n",
    "        # Link Compustat GVKEY to CRSP PERMNO\n",
    "        link_query = \"\"\"\n",
    "            SELECT lpermno AS permno, gvkey, linkdt, linkenddt\n",
    "            FROM crsp.ccmxpf_linktable\n",
    "            WHERE linktype IN ('LU', 'LC') AND linkprim IN ('P', 'C')\n",
    "        \"\"\"\n",
    "        link = db.raw_sql(link_query)\n",
    "        fund = pd.merge(fund, link, on='gvkey', how='left')\n",
    "        fund = fund.dropna(subset=['permno'])\n",
    "\n",
    "        # Sort both datasets by date\n",
    "        combined_data_sorted = combined_data.sort_values('date')\n",
    "        fund_sorted = fund.sort_values('rdq')\n",
    "        fund_sorted['permno'] = fund_sorted['permno'].astype(int)\n",
    "        combined_data_sorted['permno'] = combined_data_sorted['permno'].astype(int)\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            combined_data_sorted,\n",
    "            fund_sorted,\n",
    "            left_on='date',\n",
    "            right_on='rdq',\n",
    "            by='permno',\n",
    "            direction='backward'\n",
    "        )\n",
    "        # merged = merged.dropna(subset=['rdq', 'ticker'])\n",
    "        merged = merged.sort_values(by='date')\n",
    "        merged = merged[['permno', 'ticker', 'date', 'ret', 'prc','vol', 'mktcap', 'gvkey', 'rdq', 'saleq']]\n",
    "        merged = merged.ffill()\n",
    "\n",
    "        unique_dates = merged['date'].unique()\n",
    "        date_mapping = {date: i for i, date in enumerate(sorted(unique_dates))}\n",
    "        merged['date_mapped'] = merged['date'].map(date_mapping)\n",
    "\n",
    "        merged['year'] = pd.to_datetime(merged['date']).dt.year\n",
    "        final_data = merged[merged['year'] >= final_year]\n",
    "\n",
    "        \n",
    "        return merged, final_data\n",
    "\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        data = self.final_data\n",
    "        lookback = self.lookback\n",
    "        unique_dates = pd.to_datetime(data['date'].unique())\n",
    "        unique_dates_sorted = np.sort(unique_dates)\n",
    "        num_features = 6  # Based on []'permno', 'ret', 'prc', 'vol', 'mktcap', 'saleq']\n",
    "\n",
    "        sequences = []\n",
    "        future_returns = []\n",
    "        masks = []\n",
    "\n",
    "        for start_idx in tqdm(range(len(unique_dates_sorted) - 2 * lookback+1)):\n",
    "            hist_start = unique_dates_sorted[start_idx]\n",
    "            hist_end = unique_dates_sorted[start_idx + lookback - 1]\n",
    "            future_start = unique_dates_sorted[start_idx + lookback]\n",
    "            future_end = unique_dates_sorted[start_idx + 2 * lookback-1]\n",
    "\n",
    "            print(f'Hist start: {hist_start}, Hist end: {hist_end}, Future start: {future_start}, Future end: {future_end}')\n",
    "\n",
    "            # Initialize batch arrays with zeros\n",
    "            batch_features = np.zeros((self.global_max_assets, lookback, num_features))\n",
    "            batch_returns = np.zeros((self.global_max_assets, lookback))\n",
    "            batch_mask = np.zeros(self.global_max_assets, dtype=bool)\n",
    "\n",
    "            for permno in self.unique_permnos:\n",
    "                idx = self.permno_to_idx[permno]\n",
    "\n",
    "                # Historical data for the current window\n",
    "                hist_data = data[\n",
    "                    (data['permno'] == permno) &\n",
    "                    (data['date'] >= hist_start) &\n",
    "                    (data['date'] <= hist_end)\n",
    "                ].sort_values('date')\n",
    "\n",
    "                # Future returns for the next window\n",
    "                future_data = data[\n",
    "                    (data['permno'] == permno) &\n",
    "                    (data['date'] >= future_start) &\n",
    "                    (data['date'] <= future_end)\n",
    "                ]['ret'].values\n",
    "\n",
    "                # Check if both periods have complete data\n",
    "                if len(hist_data) == lookback and len(future_data) == lookback:\n",
    "                    features = hist_data[['permno', 'ret', 'prc', 'vol', 'mktcap', 'saleq']].values\n",
    "                    batch_features[idx] = features\n",
    "                    batch_returns[idx] = future_data\n",
    "                    batch_mask[idx] = True\n",
    "\n",
    "            sequences.append(batch_features)\n",
    "            future_returns.append(batch_returns)\n",
    "            masks.append(batch_mask)\n",
    "\n",
    "        # Convert to tensors\n",
    "        sequences_tensor = torch.tensor(np.array(sequences), dtype=torch.float32)\n",
    "\n",
    "        # NEW\n",
    "        sequences_tensor = sequences_tensor.view(-1, self.lookback, num_features)\n",
    "        future_returns_tensor = torch.tensor(np.array(future_returns), dtype=torch.float32)\n",
    "        masks_tensor = torch.tensor(np.array(masks), dtype=torch.bool)\n",
    "\n",
    "        return sequences_tensor, future_returns_tensor, masks_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.future_returns[idx], self.masks[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adharsh/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "  3%|▎         | 1/37 [00:00<00:03,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2016-01-29T00:00:00.000000000, Hist end: 2016-12-30T00:00:00.000000000, Future start: 2017-01-31T00:00:00.000000000, Future end: 2017-12-29T00:00:00.000000000\n",
      "Hist start: 2016-02-29T00:00:00.000000000, Hist end: 2017-01-31T00:00:00.000000000, Future start: 2017-02-28T00:00:00.000000000, Future end: 2018-01-31T00:00:00.000000000\n",
      "Hist start: 2016-03-31T00:00:00.000000000, Hist end: 2017-02-28T00:00:00.000000000, Future start: 2017-03-31T00:00:00.000000000, Future end: 2018-02-28T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 5/37 [00:00<00:03,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2016-04-29T00:00:00.000000000, Hist end: 2017-03-31T00:00:00.000000000, Future start: 2017-04-28T00:00:00.000000000, Future end: 2018-03-29T00:00:00.000000000\n",
      "Hist start: 2016-05-31T00:00:00.000000000, Hist end: 2017-04-28T00:00:00.000000000, Future start: 2017-05-31T00:00:00.000000000, Future end: 2018-04-30T00:00:00.000000000\n",
      "Hist start: 2016-06-30T00:00:00.000000000, Hist end: 2017-05-31T00:00:00.000000000, Future start: 2017-06-30T00:00:00.000000000, Future end: 2018-05-31T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/37 [00:00<00:02,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2016-07-29T00:00:00.000000000, Hist end: 2017-06-30T00:00:00.000000000, Future start: 2017-07-31T00:00:00.000000000, Future end: 2018-06-29T00:00:00.000000000\n",
      "Hist start: 2016-08-31T00:00:00.000000000, Hist end: 2017-07-31T00:00:00.000000000, Future start: 2017-08-31T00:00:00.000000000, Future end: 2018-07-31T00:00:00.000000000\n",
      "Hist start: 2016-09-30T00:00:00.000000000, Hist end: 2017-08-31T00:00:00.000000000, Future start: 2017-09-29T00:00:00.000000000, Future end: 2018-08-31T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 11/37 [00:01<00:02,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2016-10-31T00:00:00.000000000, Hist end: 2017-09-29T00:00:00.000000000, Future start: 2017-10-31T00:00:00.000000000, Future end: 2018-09-28T00:00:00.000000000\n",
      "Hist start: 2016-11-30T00:00:00.000000000, Hist end: 2017-10-31T00:00:00.000000000, Future start: 2017-11-30T00:00:00.000000000, Future end: 2018-10-31T00:00:00.000000000\n",
      "Hist start: 2016-12-30T00:00:00.000000000, Hist end: 2017-11-30T00:00:00.000000000, Future start: 2017-12-29T00:00:00.000000000, Future end: 2018-11-30T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 13/37 [00:01<00:02,  9.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2017-01-31T00:00:00.000000000, Hist end: 2017-12-29T00:00:00.000000000, Future start: 2018-01-31T00:00:00.000000000, Future end: 2018-12-31T00:00:00.000000000\n",
      "Hist start: 2017-02-28T00:00:00.000000000, Hist end: 2018-01-31T00:00:00.000000000, Future start: 2018-02-28T00:00:00.000000000, Future end: 2019-01-31T00:00:00.000000000\n",
      "Hist start: 2017-03-31T00:00:00.000000000, Hist end: 2018-02-28T00:00:00.000000000, Future start: 2018-03-29T00:00:00.000000000, Future end: 2019-02-28T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 16/37 [00:01<00:02,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2017-04-28T00:00:00.000000000, Hist end: 2018-03-29T00:00:00.000000000, Future start: 2018-04-30T00:00:00.000000000, Future end: 2019-03-29T00:00:00.000000000\n",
      "Hist start: 2017-05-31T00:00:00.000000000, Hist end: 2018-04-30T00:00:00.000000000, Future start: 2018-05-31T00:00:00.000000000, Future end: 2019-04-30T00:00:00.000000000\n",
      "Hist start: 2017-06-30T00:00:00.000000000, Hist end: 2018-05-31T00:00:00.000000000, Future start: 2018-06-29T00:00:00.000000000, Future end: 2019-05-31T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 19/37 [00:01<00:01,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2017-07-31T00:00:00.000000000, Hist end: 2018-06-29T00:00:00.000000000, Future start: 2018-07-31T00:00:00.000000000, Future end: 2019-06-28T00:00:00.000000000\n",
      "Hist start: 2017-08-31T00:00:00.000000000, Hist end: 2018-07-31T00:00:00.000000000, Future start: 2018-08-31T00:00:00.000000000, Future end: 2019-07-31T00:00:00.000000000\n",
      "Hist start: 2017-09-29T00:00:00.000000000, Hist end: 2018-08-31T00:00:00.000000000, Future start: 2018-09-28T00:00:00.000000000, Future end: 2019-08-30T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 23/37 [00:02<00:01,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2017-10-31T00:00:00.000000000, Hist end: 2018-09-28T00:00:00.000000000, Future start: 2018-10-31T00:00:00.000000000, Future end: 2019-09-30T00:00:00.000000000\n",
      "Hist start: 2017-11-30T00:00:00.000000000, Hist end: 2018-10-31T00:00:00.000000000, Future start: 2018-11-30T00:00:00.000000000, Future end: 2019-10-31T00:00:00.000000000\n",
      "Hist start: 2017-12-29T00:00:00.000000000, Hist end: 2018-11-30T00:00:00.000000000, Future start: 2018-12-31T00:00:00.000000000, Future end: 2019-11-29T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 25/37 [00:02<00:01,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2018-01-31T00:00:00.000000000, Hist end: 2018-12-31T00:00:00.000000000, Future start: 2019-01-31T00:00:00.000000000, Future end: 2019-12-31T00:00:00.000000000\n",
      "Hist start: 2018-02-28T00:00:00.000000000, Hist end: 2019-01-31T00:00:00.000000000, Future start: 2019-02-28T00:00:00.000000000, Future end: 2020-01-31T00:00:00.000000000\n",
      "Hist start: 2018-03-29T00:00:00.000000000, Hist end: 2019-02-28T00:00:00.000000000, Future start: 2019-03-29T00:00:00.000000000, Future end: 2020-02-28T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 28/37 [00:02<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2018-04-30T00:00:00.000000000, Hist end: 2019-03-29T00:00:00.000000000, Future start: 2019-04-30T00:00:00.000000000, Future end: 2020-03-31T00:00:00.000000000\n",
      "Hist start: 2018-05-31T00:00:00.000000000, Hist end: 2019-04-30T00:00:00.000000000, Future start: 2019-05-31T00:00:00.000000000, Future end: 2020-04-30T00:00:00.000000000\n",
      "Hist start: 2018-06-29T00:00:00.000000000, Hist end: 2019-05-31T00:00:00.000000000, Future start: 2019-06-28T00:00:00.000000000, Future end: 2020-05-29T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 31/37 [00:03<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2018-07-31T00:00:00.000000000, Hist end: 2019-06-28T00:00:00.000000000, Future start: 2019-07-31T00:00:00.000000000, Future end: 2020-06-30T00:00:00.000000000\n",
      "Hist start: 2018-08-31T00:00:00.000000000, Hist end: 2019-07-31T00:00:00.000000000, Future start: 2019-08-30T00:00:00.000000000, Future end: 2020-07-31T00:00:00.000000000\n",
      "Hist start: 2018-09-28T00:00:00.000000000, Hist end: 2019-08-30T00:00:00.000000000, Future start: 2019-09-30T00:00:00.000000000, Future end: 2020-08-31T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 34/37 [00:03<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2018-10-31T00:00:00.000000000, Hist end: 2019-09-30T00:00:00.000000000, Future start: 2019-10-31T00:00:00.000000000, Future end: 2020-09-30T00:00:00.000000000\n",
      "Hist start: 2018-11-30T00:00:00.000000000, Hist end: 2019-10-31T00:00:00.000000000, Future start: 2019-11-29T00:00:00.000000000, Future end: 2020-10-30T00:00:00.000000000\n",
      "Hist start: 2018-12-31T00:00:00.000000000, Hist end: 2019-11-29T00:00:00.000000000, Future start: 2019-12-31T00:00:00.000000000, Future end: 2020-11-30T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:03<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hist start: 2019-01-31T00:00:00.000000000, Hist end: 2019-12-31T00:00:00.000000000, Future start: 2020-01-31T00:00:00.000000000, Future end: 2020-12-31T00:00:00.000000000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 672 is out of bounds for dimension 0 with size 37",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m data \u001b[38;5;241m=\u001b[39m AlphaPortfolioData(start_year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2014\u001b[39m, end_year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2020\u001b[39m, final_year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2016\u001b[39m, lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, G\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m train_model(data, model_srem, model_caan, portfolio_generator, optimizer, criterion)\n",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data, model_srem, model_caan, portfolio_generator, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     72\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sequences, future_returns, masks \u001b[38;5;129;01min\u001b[39;00m DataLoader(data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     75\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m# Pass sequences through the SREM (to capture asset trends)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 200\u001b[0m, in \u001b[0;36mAlphaPortfolioData.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture_returns[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks[idx]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 672 is out of bounds for dimension 0 with size 37"
     ]
    }
   ],
   "source": [
    "# Define SREM Module (Transformer Encoder)\n",
    "class SREM(nn.Module):\n",
    "    # def __init__(self, input_dim, embed_dim, num_heads, num_layers):\n",
    "    #     super(SREM, self).__init__()\n",
    "    #     encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "    #     self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    #     self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.transformer(x)\n",
    "    #     x = self.fc(x.mean(dim=1))\n",
    "    #     return x\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(SREM, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Linear transformation to embedding space\n",
    "        x = self.transformer_encoder(x)  # Apply Transformer Encoder\n",
    "        x = self.output_layer(x[:, -1, :])  # Extract last time-step representation\n",
    "        return x\n",
    "\n",
    "# Define CAAN Module (Cross-Asset Attention Network)\n",
    "class CAAN(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(CAAN, self).__init__()\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attn = torch.softmax(Q @ K.transpose(-2, -1) / np.sqrt(Q.size(-1)), dim=-1)\n",
    "        attention_output = attn @ V\n",
    "        winner_scores = torch.tanh(self.fc(attention_output).squeeze(-1))\n",
    "        return winner_scores\n",
    "\n",
    "# Portfolio Generator\n",
    "class PortfolioGenerator:\n",
    "    def __init__(self, long_short_ratio=0.1):\n",
    "        self.long_short_ratio = long_short_ratio\n",
    "\n",
    "    def generate_portfolio(self, scores):\n",
    "        num_assets = len(scores)\n",
    "        num_long = num_short = int(self.long_short_ratio * num_assets)\n",
    "        \n",
    "        ranked = torch.argsort(scores, descending=True)\n",
    "        long_assets = ranked[:num_long]\n",
    "        short_assets = ranked[-num_short:]\n",
    "        \n",
    "        long_weights = torch.softmax(scores[long_assets], dim=0)\n",
    "        short_weights = torch.softmax(-scores[short_assets], dim=0)\n",
    "        \n",
    "        portfolio = torch.zeros(num_assets)\n",
    "        portfolio[long_assets] = long_weights\n",
    "        portfolio[short_assets] = -short_weights\n",
    "        \n",
    "        return portfolio\n",
    "\n",
    "\n",
    "def train_model(data, model_srem, model_caan, portfolio_generator, optimizer, criterion, num_epochs=10):\n",
    "    model_srem.train()\n",
    "    model_caan.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for sequences, future_returns, masks in DataLoader(data, batch_size=32, shuffle=True):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Pass sequences through the SREM (to capture asset trends)\n",
    "            srem_output = model_srem(sequences)\n",
    "\n",
    "            # Pass through CAAN (to model interrelationships)\n",
    "            winner_scores = model_caan(srem_output)\n",
    "\n",
    "\n",
    "            # Generate portfolio weights from the winner scores\n",
    "            portfolio = portfolio_generator.generate_portfolio(winner_scores)\n",
    "\n",
    "            # Compute loss (e.g., portfolio returns)\n",
    "            portfolio_returns = torch.sum(portfolio * future_returns, dim=1)\n",
    "            loss = -portfolio_returns.mean()  # Maximize returns (minimize negative returns)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Hyperparameters and Model Initialization\n",
    "input_dim = 6  # Features: permno, ret, prc, vol, mktcap, saleq\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "model_srem = SREM(input_dim=input_dim, embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "model_caan = CAAN(embed_dim=embed_dim)\n",
    "portfolio_generator = PortfolioGenerator(long_short_ratio=0.1)\n",
    "\n",
    "optimizer = optim.Adam(list(model_srem.parameters()) + list(model_caan.parameters()), lr=1e-4)\n",
    "criterion = nn.MSELoss()  # Placeholder; could be customized based on your goal\n",
    "\n",
    "# Get Data\n",
    "data = AlphaPortfolioData(start_year=2014, end_year=2020, final_year=2016, lookback=12, G=2)\n",
    "\n",
    "# Train the model\n",
    "train_model(data, model_srem, model_caan, portfolio_generator, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
