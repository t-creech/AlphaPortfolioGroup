{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import wrds\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "# To this:\n",
    "import torch.optim as optim\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# db = wrds.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 1. Dataset Pipeline\n",
    "# --------------------\n",
    "class AlphaPortfolioData(Dataset):\n",
    "    \n",
    "    def __init__(self, start_date='2010-01-01', end_date='2019-12-31', lookback=12, G=2):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.G = G  # Number of assets to long/short\n",
    "        self.data = self._load_wrds_data(start_date, end_date)\n",
    "        self.sequences, self.future_returns, self.masks = self._create_sequences()\n",
    "        self._validate_data_shapes()\n",
    "\n",
    "    def _load_wrds_data(self, start_date, end_date):\n",
    "        \n",
    "        # CRSP data\n",
    "        crsp_query = f\"\"\"\n",
    "        SELECT a.permno, a.date, a.ret, a.prc, a.shrout, \n",
    "            a.vol, a.cfacshr, a.altprc, a.retx\n",
    "        FROM crsp.msf AS a\n",
    "        WHERE a.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        AND a.permno IN (\n",
    "            SELECT permno FROM crsp.msenames \n",
    "            WHERE exchcd BETWEEN 1 AND 3  \n",
    "                AND shrcd IN (10, 11)       \n",
    "            )\n",
    "        \"\"\"\n",
    "        crsp_data = db.raw_sql(crsp_query)\n",
    "\n",
    "        query_ticker = \"\"\"\n",
    "            SELECT permno, namedt, nameenddt, ticker\n",
    "            FROM crsp.stocknames\n",
    "        \"\"\"\n",
    "        stocknames = db.raw_sql(query_ticker)\n",
    "        crsp_data = crsp_data.merge(stocknames.drop_duplicates(subset=['permno']), on='permno', how='left')\n",
    "        crsp_data = crsp_data.dropna(subset=['ticker'])\n",
    "\n",
    "        crsp_data['mktcap'] = (crsp_data['prc'].abs() * crsp_data['shrout'] * 1000) / 1e6  # In millions\n",
    "        crsp_data['year'] = pd.to_datetime(crsp_data['date']).dt.year\n",
    "        crsp_data = crsp_data.dropna(subset=['mktcap'])\n",
    "        mean_mktcap = crsp_data.groupby(['year', 'permno'])['mktcap'].mean().reset_index()\n",
    "        sorted_mean_mktcap = mean_mktcap.sort_values(by=['year', 'mktcap'], ascending=[True, False])\n",
    "        top_20_each_year = sorted_mean_mktcap.groupby('year').head(50)\n",
    "\n",
    "        crsp_data = top_20_each_year.merge(crsp_data, on='permno', how='left')\n",
    "        crsp_data = crsp_data.drop_duplicates(subset=['mktcap_y'])\n",
    "\n",
    "        crsp_data = crsp_data[['permno', 'ticker', 'date', 'ret', 'prc', 'shrout', 'vol', 'mktcap_y', 'year_y']]\n",
    "        crsp_data['date'] = pd.to_datetime(crsp_data['date'])\n",
    "        crsp_data.sort_values(['permno', 'date'], inplace=True)\n",
    "\n",
    "        # Query Compustat quarterly data with release dates (rdq)\n",
    "        fund_query = f\"\"\"\n",
    "            SELECT gvkey, datadate, rdq, saleq\n",
    "            FROM comp.fundq\n",
    "            WHERE indfmt = 'INDL' AND datafmt = 'STD' AND popsrc = 'D' AND consol = 'C'\n",
    "            AND datadate BETWEEN '{start_date}' AND '{end_date}'\n",
    "            AND rdq IS NOT NULL\n",
    "        \"\"\"\n",
    "        fund = db.raw_sql(fund_query)\n",
    "        fund['rdq'] = pd.to_datetime(fund['rdq'])\n",
    "        fund['datadate'] = pd.to_datetime(fund['datadate'])\n",
    "\n",
    "        # Link Compustat GVKEY to CRSP PERMNO\n",
    "        link_query = \"\"\"\n",
    "            SELECT lpermno AS permno, gvkey, linkdt, linkenddt\n",
    "            FROM crsp.ccmxpf_linktable\n",
    "            WHERE linktype IN ('LU', 'LC') AND linkprim IN ('P', 'C')\n",
    "        \"\"\"\n",
    "        link = db.raw_sql(link_query)\n",
    "        fund = pd.merge(fund, link, on='gvkey', how='left')\n",
    "        fund = fund.dropna(subset=['permno'])\n",
    "\n",
    "        # Sort both datasets by date\n",
    "        crsp_sorted = crsp_data.sort_values('date')\n",
    "        fund_sorted = fund.sort_values('rdq')\n",
    "        fund_sorted['permno'] = fund_sorted['permno'].astype(int)\n",
    "        # Merge fundamentals to CRSP using rdq\n",
    "        merged = pd.merge_asof(\n",
    "            crsp_sorted,\n",
    "            fund_sorted,\n",
    "            left_on='date',\n",
    "            right_on='rdq',\n",
    "            by='permno',\n",
    "            direction='backward'  # Take the first CRSP date >= rdq\n",
    "        )\n",
    "        merged = merged.dropna(subset=['rdq', 'ticker'])\n",
    "        merged = merged.sort_values(by='date')\n",
    "        merged = merged[['permno', 'ticker', 'date', 'ret', 'prc','vol', 'mktcap_y', 'gvkey', 'rdq', 'saleq']]\n",
    "        merged = merged.ffill()\n",
    "        \n",
    "        unique_dates = merged['date'].unique()\n",
    "        date_mapping = {date: i for i, date in enumerate(sorted(unique_dates))}\n",
    "        merged['date_mapped'] = merged['date'].map(date_mapping)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        data = self.data\n",
    "        lookback = self.lookback\n",
    "        unique_dates = pd.to_datetime(data['date'].unique())\n",
    "        unique_assets = data['permno'].unique()\n",
    "        \n",
    "        sequences = []\n",
    "        future_returns = []\n",
    "        masks = []\n",
    "        min_assets = 2 * self.G\n",
    "        batch_info = []\n",
    "\n",
    "        # First pass: collect valid batches\n",
    "        for date_idx in range(len(unique_dates) - 2 * lookback):\n",
    "            hist_start = unique_dates[date_idx]\n",
    "            hist_end = unique_dates[date_idx + lookback - 1]\n",
    "            future_start = unique_dates[date_idx + lookback]\n",
    "            future_end = unique_dates[date_idx + 2 * lookback - 1]\n",
    "\n",
    "            batch_assets = []\n",
    "            hist_features = []\n",
    "            fwd_returns = []\n",
    "            \n",
    "            for asset in unique_assets:\n",
    "                asset_hist = data[\n",
    "                    (data['permno'] == asset) & \n",
    "                    (data['date'].between(hist_start, hist_end))\n",
    "                ].sort_values('date')\n",
    "                \n",
    "                asset_future = data[\n",
    "                    (data['permno'] == asset) & \n",
    "                    (data['date'].between(future_start, future_end))\n",
    "                ]['ret'].values\n",
    "                \n",
    "                if len(asset_hist) == lookback and len(asset_future) == lookback:\n",
    "                    features = asset_hist[['ret', 'prc', 'vol', 'mktcap_y', 'saleq']].values\n",
    "                    hist_features.append(features)\n",
    "                    fwd_returns.append(asset_future)\n",
    "                    batch_assets.append(asset)\n",
    "\n",
    "            if len(hist_features) >= min_assets:\n",
    "                batch_info.append({\n",
    "                    'features': np.stack(hist_features),\n",
    "                    'returns': np.stack(fwd_returns),\n",
    "                    'num_assets': len(hist_features)\n",
    "                })\n",
    "\n",
    "        # Find global max assets across all valid batches\n",
    "        if not batch_info:\n",
    "            return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "        \n",
    "        global_max_assets = max(b['num_assets'] for b in batch_info)\n",
    "        features_dim = batch_info[0]['features'].shape[-1]\n",
    "\n",
    "        # Second pass: pad to global max\n",
    "        for batch in batch_info:\n",
    "            num_assets = batch['num_assets']\n",
    "            \n",
    "            # Features: (assets, lookback, features)\n",
    "            padded_features = np.zeros((global_max_assets, lookback, features_dim))\n",
    "            padded_features[:num_assets] = batch['features']\n",
    "            \n",
    "            # Returns: (assets, lookback)\n",
    "            padded_returns = np.zeros((global_max_assets, lookback))  # Fix 1: 2D padding\n",
    "            padded_returns[:num_assets] = batch['returns']\n",
    "            \n",
    "            # Mask: (assets,)\n",
    "            mask = np.zeros(global_max_assets, dtype=bool)\n",
    "            mask[:num_assets] = True\n",
    "\n",
    "            sequences.append(padded_features)\n",
    "            future_returns.append(padded_returns)\n",
    "            masks.append(mask)\n",
    "\n",
    "        return (\n",
    "            torch.as_tensor(np.array(sequences), dtype=torch.float32),  # (time, assets, lookback, features)\n",
    "            torch.as_tensor(np.array(future_returns), dtype=torch.float32),  # (time, assets, lookback)\n",
    "            torch.as_tensor(np.array(masks), dtype=torch.bool)  # (time, assets)\n",
    "        )\n",
    "\n",
    "    def _validate_data_shapes(self):\n",
    "        assert self.sequences.dim() == 4, \\\n",
    "            f\"Sequences should be 4D (time, assets, lookback, features). Got {self.sequences.shape}\"\n",
    "        assert self.future_returns.dim() == 3, \\\n",
    "            f\"Future returns should be 3D (time, assets, lookback). Got {self.future_returns.shape}\"\n",
    "        assert self.masks.dim() == 2, \\\n",
    "            f\"Masks should be 2D (time, assets). Got {self.masks.shape}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "        self.sequences[idx],  # (assets, lookback, features)\n",
    "        self.future_returns[idx],  # (assets, lookback)\n",
    "        self.masks[idx]         # (assets,)\n",
    "    )\n",
    "\n",
    "class AssetTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=4*d_model)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_assets, lookback, features = x.shape\n",
    "        x = x.view(-1, lookback, features)  # Reshape to (batch*assets, lookback, features)\n",
    "        x = self.embedding(x)  # Project to d_model\n",
    "        x = x.permute(1, 0, 2)  # (lookback, batch*assets, d_model)\n",
    "        x = self.transformer(x)  # Apply transformer\n",
    "        x = x.mean(dim=0).view(batch_size, num_assets, -1)  # Global average pooling\n",
    "        return x\n",
    "\n",
    "class CrossAssetAttention(nn.Module):\n",
    "    def __init__(self, d_model=64, d_k=32, d_v=32):\n",
    "        super().__init__()\n",
    "        self.WQ = nn.Linear(d_model, d_k)\n",
    "        self.WK = nn.Linear(d_model, d_k)\n",
    "        self.WV = nn.Linear(d_model, d_v)\n",
    "        self.scale = 1 / math.sqrt(d_k)\n",
    "        self.score_layer = nn.Sequential(\n",
    "            nn.Linear(d_v, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Print input shapes for debugging\n",
    "        # print(\"CrossAssetAttention Input x shape:\", x.shape)\n",
    "        # print(\"CrossAssetAttention Input mask shape:\", mask.shape)\n",
    "\n",
    "        # Ensure x is 3D (batch, assets, d_model)\n",
    "        if x.dim() == 4:\n",
    "            x = x.squeeze(0)  # Remove extra batch dimension if present\n",
    "        \n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D input, got {x.dim()}D tensor. Shape: {x.shape}\")\n",
    "\n",
    "        # Compute query, key, value\n",
    "        Q = self.WQ(x)  # (batch, assets, d_k)\n",
    "        K = self.WK(x)  # (batch, assets, d_k)\n",
    "        V = self.WV(x)  # (batch, assets, d_v)\n",
    "        \n",
    "        # Attention computation\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (batch, assets, assets)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Ensure mask is 2D\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.squeeze(0)\n",
    "            \n",
    "            if mask.dim() != 2:\n",
    "                raise ValueError(f\"Mask must be 2D, got {mask.dim()}D tensor\")\n",
    "            \n",
    "            attn = attn.masked_fill(~mask.unsqueeze(1), float('-inf'))\n",
    "        \n",
    "        # Softmax and value aggregation\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn_out = torch.matmul(attn, V)  # (batch, assets, d_v)\n",
    "        \n",
    "        # Score computation\n",
    "        scores = self.score_layer(attn_out).squeeze(-1)  # (batch, assets)\n",
    "        \n",
    "        # print(\"CrossAssetAttention Output scores shape:\", scores.shape)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "class PortfolioGenerator(nn.Module):\n",
    "    def __init__(self, G=5):\n",
    "        super().__init__()\n",
    "        self.G = G\n",
    "        self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def forward(self, scores, mask):\n",
    "        # print(\"PortfolioGenerator Input scores shape:\", scores.shape)\n",
    "        # print(\"PortfolioGenerator Input mask shape:\", mask.shape)\n",
    "\n",
    "        # Ensure scores is 2D\n",
    "        if scores.dim() > 2:\n",
    "            scores = scores.squeeze()\n",
    "        \n",
    "        if scores.dim() != 2:\n",
    "            raise ValueError(f\"Scores must be 2D, got {scores.dim()}D tensor. Shape: {scores.shape}\")\n",
    "        \n",
    "        # Ensure mask is 2D\n",
    "        if mask.dim() > 2:\n",
    "            mask = mask.squeeze()\n",
    "        \n",
    "        if mask.dim() != 2:\n",
    "            raise ValueError(f\"Mask must be 2D, got {mask.dim()}D tensor\")\n",
    "        \n",
    "        # Validate shapes\n",
    "        batch_size, num_assets = scores.shape\n",
    "        weights = torch.zeros_like(scores)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # More robust validity check\n",
    "            valid_assets = mask[i].sum().item()\n",
    "            \n",
    "            if valid_assets < 2*self.G:\n",
    "                print(f\"Warning: Batch {i} has insufficient valid assets. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Identify top and bottom G assets\n",
    "            top_indices = torch.topk(scores[i], min(self.G, valid_assets)).indices\n",
    "            bottom_indices = torch.topk(-scores[i], min(self.G, valid_assets)).indices\n",
    "            \n",
    "            # Compute weights\n",
    "            top_scores = scores[i, top_indices]\n",
    "            bottom_scores = scores[i, bottom_indices]\n",
    "            \n",
    "            top_weights = F.softmax(top_scores / self.temperature.clamp(min=0.1), dim=-1)\n",
    "            bottom_weights = F.softmax(-bottom_scores / self.temperature.clamp(min=0.1), dim=-1)\n",
    "            \n",
    "            weights[i, top_indices] = top_weights\n",
    "            weights[i, bottom_indices] = -bottom_weights\n",
    "        \n",
    "        # print(\"PortfolioGenerator Output weights shape:\", weights.shape)\n",
    "        return weights.nan_to_num(0.0)\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, transformer, attention, portfolio_gen, d_model=64):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        self.attention = attention\n",
    "        self.portfolio_gen = portfolio_gen\n",
    "        self.log_std = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Print input shapes for debugging\n",
    "        # print(\"PolicyNetwork Input x shape:\", x.shape)\n",
    "        # print(\"PolicyNetwork Input mask shape:\", mask.shape)\n",
    "\n",
    "        # Ensure correct input dimensions\n",
    "        if x.dim() == 5:  # Extra batch dimension\n",
    "            x = x.squeeze(0)\n",
    "        \n",
    "        # if x.dim() != 4:\n",
    "            # raise ValueError(f\"Expected 4D input tensor, got {x.dim()}D. Shape: {x.shape}\")\n",
    "        \n",
    "        # Process through transformer\n",
    "        x = self.transformer(x)  # (batch, assets, d_model)\n",
    "        \n",
    "        # Ensure mask is consistent\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.squeeze(0)\n",
    "        \n",
    "        # if mask.dim() != 2:\n",
    "            # raise ValueError(f\"Expected 2D mask, got {mask.dim()}D. Shape: {mask.shape}\")\n",
    "        \n",
    "        # Cross-asset attention scoring\n",
    "        scores = self.attention(x, mask)  # (batch, assets)\n",
    "        \n",
    "        # Portfolio generation\n",
    "        weights = self.portfolio_gen(scores, mask)  # (batch, assets)\n",
    "        \n",
    "        # Policy distribution\n",
    "        std = torch.exp(self.log_std).expand_as(weights)\n",
    "        dist = torch.distributions.Normal(weights, std)\n",
    "        \n",
    "        # Value estimation\n",
    "        values = self.value_head(x.mean(dim=1))\n",
    "        \n",
    "        return dist, values.squeeze(-1)\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, sequences, future_returns, masks, G=5):\n",
    "        super().__init__()\n",
    "        self.sequences = sequences\n",
    "        self.future_returns = future_returns\n",
    "        self.masks = masks\n",
    "        self.G = G\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=sequences[0].shape)\n",
    "        self.action_space = gym.spaces.Box(-1, 1, shape=(sequences.shape[1],))\n",
    "\n",
    "    def step(self, action):\n",
    "        if np.random.random() < 0.01:  # Random sampling mechanism\n",
    "            idx = np.random.randint(len(self.sequences))\n",
    "            \n",
    "            # Get future returns and mask\n",
    "            future_returns = self.future_returns[idx]\n",
    "            mask = self.masks[idx]\n",
    "            \n",
    "            # Ensure action is a tensor\n",
    "            action = torch.as_tensor(action, dtype=torch.float32)\n",
    "            \n",
    "            # Filter valid assets\n",
    "            valid = mask & (action != 0)\n",
    "            if valid.sum().item() < 2*self.G:\n",
    "                return None, -1.0, True, {}\n",
    "            \n",
    "            # Selected weights and returns\n",
    "            selected_weights = action[valid]\n",
    "            selected_returns = future_returns[valid]\n",
    "            \n",
    "            # Portfolio return calculation\n",
    "            portfolio_returns = (selected_weights.unsqueeze(-1) * selected_returns).sum(dim=0)\n",
    "            \n",
    "            # Sharpe Ratio calculation\n",
    "            sharpe = portfolio_returns.mean() / (portfolio_returns.std() + 1e-6)\n",
    "            \n",
    "            print(sharpe)\n",
    "            \n",
    "            return None, sharpe.item(), True, {}\n",
    "        \n",
    "        return None, 0.0, True, {}\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(self, policy, env, lr=1e-4, gamma=0.99, clip=0.2):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip = clip\n",
    "    \n",
    "    def collect_trajectory(self):\n",
    "        states, masks, actions, log_probs, rewards = [], [], [], [], []\n",
    "        \n",
    "        # Determine the batch size dynamically\n",
    "        batch_size = len(self.env.sequences)\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            idx = np.random.randint(len(self.env.sequences))\n",
    "            state = self.env.sequences[idx]\n",
    "            mask = self.env.masks[idx]\n",
    "            \n",
    "            # Ensure consistent dimensions\n",
    "            if state.dim() == 3:\n",
    "                state = state.unsqueeze(0)  # Add batch dimension\n",
    "            elif state.dim() == 4:\n",
    "                state = state.squeeze(0)  # Remove extra dimension if present\n",
    "            \n",
    "            if mask.dim() == 1:\n",
    "                mask = mask.unsqueeze(0)  # Add batch dimension\n",
    "            elif mask.dim() == 2:\n",
    "                mask = mask.squeeze(0)  # Remove extra dimension if present\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    # Ensure state and mask have consistent batch dimension\n",
    "                    dist, values = self.policy(state, mask)\n",
    "                    action = dist.sample()\n",
    "                    log_prob = dist.log_prob(action)\n",
    "                    \n",
    "                    _, reward, _, _ = self.env.step(action.squeeze())\n",
    "                    \n",
    "                    states.append(state.squeeze(0))\n",
    "                    masks.append(mask.squeeze(0))\n",
    "                    actions.append(action.squeeze(0))\n",
    "                    log_probs.append(log_prob.squeeze(0))\n",
    "                    rewards.append(reward)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing trajectory: {e}\")\n",
    "                    # print(\"State shape:\", state.shape)\n",
    "                    # print(\"Mask shape:\", mask.shape)\n",
    "                    raise\n",
    "        \n",
    "        # Ensure all tensors have consistent shapes\n",
    "        states = torch.stack(states)\n",
    "        masks = torch.stack(masks)\n",
    "        actions = torch.stack(actions)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        \n",
    "        return states, masks, actions, log_probs, rewards\n",
    "\n",
    "    def train_epoch(self):\n",
    "        states, masks, actions, old_log_probs, rewards = self.collect_trajectory()\n",
    "        \n",
    "        # Ensure rewards have the correct shape for broadcasting\n",
    "        rewards = rewards.view(-1, 1)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
    "        \n",
    "        for _ in range(3):  # Multiple PPO update steps\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Get new policy outputs\n",
    "            dist, values = self.policy(states, masks)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Ensure consistent shapes\n",
    "            values = values.view(-1, 1)\n",
    "            \n",
    "            # Policy loss computation\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            surr1 = ratio * rewards\n",
    "            surr2 = torch.clamp(ratio, 1-self.clip, 1+self.clip) * rewards\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(values, rewards)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5*value_loss - 0.01*entropy\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AlphaPortfolioData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = AlphaPortfolioData()\n",
    "time_steps, num_assets, lookback, features = dataset.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0819)\n",
      "tensor(0.5421, grad_fn=<SubBackward0>)\n",
      "tensor(0.5010, grad_fn=<SubBackward0>)\n",
      "tensor(0.4824, grad_fn=<SubBackward0>)\n",
      "Epoch 1/30 | Loss: 0.4824\n",
      "tensor(-0.0379)\n",
      "tensor(0.4852, grad_fn=<SubBackward0>)\n",
      "tensor(0.4921, grad_fn=<SubBackward0>)\n",
      "tensor(0.4914, grad_fn=<SubBackward0>)\n",
      "Epoch 2/30 | Loss: 0.4914\n",
      "tensor(-0.1996)\n",
      "tensor(0.4877, grad_fn=<SubBackward0>)\n",
      "tensor(0.4818, grad_fn=<SubBackward0>)\n",
      "tensor(0.4805, grad_fn=<SubBackward0>)\n",
      "Epoch 3/30 | Loss: 0.4805\n",
      "tensor(-0.5251)\n",
      "tensor(0.4823, grad_fn=<SubBackward0>)\n",
      "tensor(0.4834, grad_fn=<SubBackward0>)\n",
      "tensor(0.4848, grad_fn=<SubBackward0>)\n",
      "Epoch 4/30 | Loss: 0.4848\n",
      "tensor(0.0603)\n",
      "tensor(0.4852, grad_fn=<SubBackward0>)\n",
      "tensor(0.4838, grad_fn=<SubBackward0>)\n",
      "tensor(0.4820, grad_fn=<SubBackward0>)\n",
      "Epoch 5/30 | Loss: 0.4820\n",
      "tensor(-0.6513)\n",
      "tensor(-0.0492)\n",
      "tensor(0.2825)\n",
      "tensor(0.4805, grad_fn=<SubBackward0>)\n",
      "tensor(0.4818, grad_fn=<SubBackward0>)\n",
      "tensor(0.4842, grad_fn=<SubBackward0>)\n",
      "Epoch 6/30 | Loss: 0.4842\n",
      "tensor(-0.0106, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0116, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0130, grad_fn=<SubBackward0>)\n",
      "Epoch 7/30 | Loss: -0.0130\n",
      "tensor(-0.0140, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0140, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0133, grad_fn=<SubBackward0>)\n",
      "Epoch 8/30 | Loss: -0.0133\n",
      "tensor(0.3365)\n",
      "tensor(0.4825, grad_fn=<SubBackward0>)\n",
      "tensor(0.4835, grad_fn=<SubBackward0>)\n",
      "tensor(0.4827, grad_fn=<SubBackward0>)\n",
      "Epoch 9/30 | Loss: 0.4827\n",
      "tensor(-0.0136, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "Epoch 10/30 | Loss: -0.0141\n",
      "tensor(-0.2427)\n",
      "tensor(-0.1221)\n",
      "tensor(0.4110)\n",
      "tensor(0.4835, grad_fn=<SubBackward0>)\n",
      "tensor(0.4827, grad_fn=<SubBackward0>)\n",
      "tensor(0.4834, grad_fn=<SubBackward0>)\n",
      "Epoch 11/30 | Loss: 0.4834\n",
      "tensor(-0.0134, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0138, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "Epoch 12/30 | Loss: -0.0142\n",
      "tensor(0.1116)\n",
      "tensor(0.7768)\n",
      "tensor(0.4824, grad_fn=<SubBackward0>)\n",
      "tensor(0.4808, grad_fn=<SubBackward0>)\n",
      "tensor(0.4810, grad_fn=<SubBackward0>)\n",
      "Epoch 13/30 | Loss: 0.4810\n",
      "tensor(-0.0137, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0139, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "Epoch 14/30 | Loss: -0.0141\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "Epoch 15/30 | Loss: -0.0141\n",
      "tensor(-0.2696)\n",
      "tensor(0.4835, grad_fn=<SubBackward0>)\n",
      "tensor(0.4817, grad_fn=<SubBackward0>)\n",
      "tensor(0.4822, grad_fn=<SubBackward0>)\n",
      "Epoch 16/30 | Loss: 0.4822\n",
      "tensor(-0.3145)\n",
      "tensor(0.4824, grad_fn=<SubBackward0>)\n",
      "tensor(0.4798, grad_fn=<SubBackward0>)\n",
      "tensor(0.4809, grad_fn=<SubBackward0>)\n",
      "Epoch 17/30 | Loss: 0.4809\n",
      "tensor(0.0007)\n",
      "tensor(0.4685, grad_fn=<SubBackward0>)\n",
      "tensor(0.4676, grad_fn=<SubBackward0>)\n",
      "tensor(0.4691, grad_fn=<SubBackward0>)\n",
      "Epoch 18/30 | Loss: 0.4691\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "Epoch 19/30 | Loss: -0.0141\n",
      "tensor(-0.0140, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0140, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "Epoch 20/30 | Loss: -0.0141\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "Epoch 21/30 | Loss: -0.0142\n",
      "tensor(-0.2545)\n",
      "tensor(0.4800, grad_fn=<SubBackward0>)\n",
      "tensor(0.4799, grad_fn=<SubBackward0>)\n",
      "tensor(0.4796, grad_fn=<SubBackward0>)\n",
      "Epoch 22/30 | Loss: 0.4796\n",
      "tensor(0.1483)\n",
      "tensor(0.4808, grad_fn=<SubBackward0>)\n",
      "tensor(0.4808, grad_fn=<SubBackward0>)\n",
      "tensor(0.4809, grad_fn=<SubBackward0>)\n",
      "Epoch 23/30 | Loss: 0.4809\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "Epoch 24/30 | Loss: -0.0141\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "Epoch 25/30 | Loss: -0.0142\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0141, grad_fn=<SubBackward0>)\n",
      "Epoch 26/30 | Loss: -0.0141\n",
      "tensor(-0.1005)\n",
      "tensor(-0.3356)\n",
      "tensor(-0.5318)\n",
      "tensor(0.4838, grad_fn=<SubBackward0>)\n",
      "tensor(0.4825, grad_fn=<SubBackward0>)\n",
      "tensor(0.4821, grad_fn=<SubBackward0>)\n",
      "Epoch 27/30 | Loss: 0.4821\n",
      "tensor(0.2252)\n",
      "tensor(0.4807, grad_fn=<SubBackward0>)\n",
      "tensor(0.4797, grad_fn=<SubBackward0>)\n",
      "tensor(0.4809, grad_fn=<SubBackward0>)\n",
      "Epoch 28/30 | Loss: 0.4809\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0142, grad_fn=<SubBackward0>)\n",
      "Epoch 29/30 | Loss: -0.0142\n",
      "tensor(0.3540)\n",
      "tensor(-0.1138)\n",
      "tensor(0.4804, grad_fn=<SubBackward0>)\n",
      "tensor(0.4806, grad_fn=<SubBackward0>)\n",
      "tensor(0.4803, grad_fn=<SubBackward0>)\n",
      "Epoch 30/30 | Loss: 0.4803\n"
     ]
    }
   ],
   "source": [
    "d_model = 128\n",
    "G = 4  # Top and bottom K stocks\n",
    "epochs = 30\n",
    "\n",
    "# Load your preprocessed data from AlphaPortfolioData\n",
    "\n",
    "\n",
    "# Network Components\n",
    "transformer = AssetTransformer(input_dim=features, d_model=d_model)\n",
    "attention = CrossAssetAttention(d_model=d_model)\n",
    "portfolio_gen = PortfolioGenerator(G=G)\n",
    "\n",
    "# Policy Network\n",
    "policy = PolicyNetwork(transformer, attention, portfolio_gen, d_model=d_model)\n",
    "\n",
    "# Portfolio Environment\n",
    "env = PortfolioEnv(\n",
    "    dataset.sequences, \n",
    "    dataset.future_returns, \n",
    "    dataset.masks, \n",
    "    G=G\n",
    ")\n",
    "\n",
    "# PPO Trainer\n",
    "trainer = PPOTrainer(policy, env)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_epoch()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 1. Dataset Pipeline\n",
    "# --------------------\n",
    "class AlphaPortfolioData(Dataset):\n",
    "    \n",
    "    def __init__(self, start_date='2010-01-01', end_date='2019-12-31', lookback=12, G=2):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.G = G  # Number of assets to long/short\n",
    "        self.data = self._load_wrds_data(start_date, end_date)\n",
    "        self.sequences, self.future_returns, self.masks = self._create_sequences()\n",
    "        self._validate_data_shapes()\n",
    "\n",
    "    def _load_wrds_data(self, start_date, end_date):\n",
    "        \n",
    "        # CRSP data\n",
    "        crsp_query = f\"\"\"\n",
    "        SELECT a.permno, a.date, a.ret, a.prc, a.shrout, \n",
    "            a.vol, a.cfacshr, a.altprc, a.retx\n",
    "        FROM crsp.msf AS a\n",
    "        WHERE a.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        AND a.permno IN (\n",
    "            SELECT permno FROM crsp.msenames \n",
    "            WHERE exchcd BETWEEN 1 AND 3  \n",
    "                AND shrcd IN (10, 11)       \n",
    "            )\n",
    "        \"\"\"\n",
    "        crsp_data = db.raw_sql(crsp_query)\n",
    "\n",
    "        query_ticker = \"\"\"\n",
    "            SELECT permno, namedt, nameenddt, ticker\n",
    "            FROM crsp.stocknames\n",
    "        \"\"\"\n",
    "        stocknames = db.raw_sql(query_ticker)\n",
    "        crsp_data = crsp_data.merge(stocknames.drop_duplicates(subset=['permno']), on='permno', how='left')\n",
    "        crsp_data = crsp_data.dropna(subset=['ticker'])\n",
    "\n",
    "        crsp_data['mktcap'] = (crsp_data['prc'].abs() * crsp_data['shrout'] * 1000) / 1e6  # In millions\n",
    "        crsp_data['year'] = pd.to_datetime(crsp_data['date']).dt.year\n",
    "        crsp_data = crsp_data.dropna(subset=['mktcap'])\n",
    "        mean_mktcap = crsp_data.groupby(['year', 'permno'])['mktcap'].mean().reset_index()\n",
    "        sorted_mean_mktcap = mean_mktcap.sort_values(by=['year', 'mktcap'], ascending=[True, False])\n",
    "        top_20_each_year = sorted_mean_mktcap.groupby('year').head(50)\n",
    "\n",
    "        crsp_data = top_20_each_year.merge(crsp_data, on='permno', how='left')\n",
    "        crsp_data = crsp_data.drop_duplicates(subset=['mktcap_y'])\n",
    "\n",
    "        crsp_data = crsp_data[['permno', 'ticker', 'date', 'ret', 'prc', 'shrout', 'vol', 'mktcap_y', 'year_y']]\n",
    "        crsp_data['date'] = pd.to_datetime(crsp_data['date'])\n",
    "        crsp_data.sort_values(['permno', 'date'], inplace=True)\n",
    "\n",
    "        # Query Compustat quarterly data with release dates (rdq)\n",
    "        fund_query = f\"\"\"\n",
    "            SELECT gvkey, datadate, rdq, saleq\n",
    "            FROM comp.fundq\n",
    "            WHERE indfmt = 'INDL' AND datafmt = 'STD' AND popsrc = 'D' AND consol = 'C'\n",
    "            AND datadate BETWEEN '{start_date}' AND '{end_date}'\n",
    "            AND rdq IS NOT NULL\n",
    "        \"\"\"\n",
    "        fund = db.raw_sql(fund_query)\n",
    "        fund['rdq'] = pd.to_datetime(fund['rdq'])\n",
    "        fund['datadate'] = pd.to_datetime(fund['datadate'])\n",
    "\n",
    "        # Link Compustat GVKEY to CRSP PERMNO\n",
    "        link_query = \"\"\"\n",
    "            SELECT lpermno AS permno, gvkey, linkdt, linkenddt\n",
    "            FROM crsp.ccmxpf_linktable\n",
    "            WHERE linktype IN ('LU', 'LC') AND linkprim IN ('P', 'C')\n",
    "        \"\"\"\n",
    "        link = db.raw_sql(link_query)\n",
    "        fund = pd.merge(fund, link, on='gvkey', how='left')\n",
    "        fund = fund.dropna(subset=['permno'])\n",
    "\n",
    "        # Sort both datasets by date\n",
    "        crsp_sorted = crsp_data.sort_values('date')\n",
    "        fund_sorted = fund.sort_values('rdq')\n",
    "        fund_sorted['permno'] = fund_sorted['permno'].astype(int)\n",
    "        # Merge fundamentals to CRSP using rdq\n",
    "        merged = pd.merge_asof(\n",
    "            crsp_sorted,\n",
    "            fund_sorted,\n",
    "            left_on='date',\n",
    "            right_on='rdq',\n",
    "            by='permno',\n",
    "            direction='backward'  # Take the first CRSP date >= rdq\n",
    "        )\n",
    "        merged = merged.dropna(subset=['rdq', 'ticker'])\n",
    "        merged = merged.sort_values(by='date')\n",
    "        merged = merged[['permno', 'ticker', 'date', 'ret', 'prc','vol', 'mktcap_y', 'gvkey', 'rdq', 'saleq']]\n",
    "        merged = merged.ffill()\n",
    "        \n",
    "        unique_dates = merged['date'].unique()\n",
    "        date_mapping = {date: i for i, date in enumerate(sorted(unique_dates))}\n",
    "        merged['date_mapped'] = merged['date'].map(date_mapping)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        data = self.data\n",
    "        lookback = self.lookback\n",
    "        unique_dates = pd.to_datetime(data['date'].unique())\n",
    "        unique_assets = data['permno'].unique()\n",
    "        \n",
    "        sequences = []\n",
    "        future_returns = []\n",
    "        masks = []\n",
    "        min_assets = 2 * self.G\n",
    "        batch_info = []\n",
    "\n",
    "        # First pass: collect valid batches\n",
    "        for date_idx in range(len(unique_dates) - 2 * lookback):\n",
    "            hist_start = unique_dates[date_idx]\n",
    "            hist_end = unique_dates[date_idx + lookback - 1]\n",
    "            future_start = unique_dates[date_idx + lookback]\n",
    "            future_end = unique_dates[date_idx + 2 * lookback - 1]\n",
    "\n",
    "            batch_assets = []\n",
    "            hist_features = []\n",
    "            fwd_returns = []\n",
    "            \n",
    "            for asset in unique_assets:\n",
    "                asset_hist = data[\n",
    "                    (data['permno'] == asset) & \n",
    "                    (data['date'].between(hist_start, hist_end))\n",
    "                ].sort_values('date')\n",
    "                \n",
    "                asset_future = data[\n",
    "                    (data['permno'] == asset) & \n",
    "                    (data['date'].between(future_start, future_end))\n",
    "                ]['ret'].values\n",
    "                \n",
    "                if len(asset_hist) == lookback and len(asset_future) == lookback:\n",
    "                    features = asset_hist[['ret', 'prc', 'vol', 'mktcap_y', 'saleq']].values\n",
    "                    hist_features.append(features)\n",
    "                    fwd_returns.append(asset_future)\n",
    "                    batch_assets.append(asset)\n",
    "\n",
    "            if len(hist_features) >= min_assets:\n",
    "                batch_info.append({\n",
    "                    'features': np.stack(hist_features),\n",
    "                    'returns': np.stack(fwd_returns),\n",
    "                    'num_assets': len(hist_features)\n",
    "                })\n",
    "\n",
    "        # Find global max assets across all valid batches\n",
    "        if not batch_info:\n",
    "            return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "        \n",
    "        global_max_assets = max(b['num_assets'] for b in batch_info)\n",
    "        features_dim = batch_info[0]['features'].shape[-1]\n",
    "\n",
    "        # Second pass: pad to global max\n",
    "        for batch in batch_info:\n",
    "            num_assets = batch['num_assets']\n",
    "            \n",
    "            # Features: (assets, lookback, features)\n",
    "            padded_features = np.zeros((global_max_assets, lookback, features_dim))\n",
    "            padded_features[:num_assets] = batch['features']\n",
    "            \n",
    "            # Returns: (assets, lookback)\n",
    "            padded_returns = np.zeros((global_max_assets, lookback))  # Fix 1: 2D padding\n",
    "            padded_returns[:num_assets] = batch['returns']\n",
    "            \n",
    "            # Mask: (assets,)\n",
    "            mask = np.zeros(global_max_assets, dtype=bool)\n",
    "            mask[:num_assets] = True\n",
    "\n",
    "            sequences.append(padded_features)\n",
    "            future_returns.append(padded_returns)\n",
    "            masks.append(mask)\n",
    "\n",
    "        return (\n",
    "            torch.as_tensor(np.array(sequences), dtype=torch.float32),  # (time, assets, lookback, features)\n",
    "            torch.as_tensor(np.array(future_returns), dtype=torch.float32),  # (time, assets, lookback)\n",
    "            torch.as_tensor(np.array(masks), dtype=torch.bool)  # (time, assets)\n",
    "        )\n",
    "\n",
    "    def _validate_data_shapes(self):\n",
    "        assert self.sequences.dim() == 4, \\\n",
    "            f\"Sequences should be 4D (time, assets, lookback, features). Got {self.sequences.shape}\"\n",
    "        assert self.future_returns.dim() == 3, \\\n",
    "            f\"Future returns should be 3D (time, assets, lookback). Got {self.future_returns.shape}\"\n",
    "        assert self.masks.dim() == 2, \\\n",
    "            f\"Masks should be 2D (time, assets). Got {self.masks.shape}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "        self.sequences[idx],  # (assets, lookback, features)\n",
    "        self.future_returns[idx],  # (assets, lookback)\n",
    "        self.masks[idx]         # (assets,)\n",
    "    )\n",
    "\n",
    "class AssetTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=4*d_model)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_assets, lookback, features = x.shape\n",
    "        x = x.view(-1, lookback, features)  # Reshape to (batch*assets, lookback, features)\n",
    "        x = self.embedding(x)  # Project to d_model\n",
    "        x = x.permute(1, 0, 2)  # (lookback, batch*assets, d_model)\n",
    "        x = self.transformer(x)  # Apply transformer\n",
    "        x = x.mean(dim=0).view(batch_size, num_assets, -1)  # Global average pooling\n",
    "        return x\n",
    "\n",
    "class CrossAssetAttention(nn.Module):\n",
    "    def __init__(self, d_model=64, d_k=32, d_v=32):\n",
    "        super().__init__()\n",
    "        self.WQ = nn.Linear(d_model, d_k)\n",
    "        self.WK = nn.Linear(d_model, d_k)\n",
    "        self.WV = nn.Linear(d_model, d_v)\n",
    "        self.scale = 1 / math.sqrt(d_k)\n",
    "        self.score_layer = nn.Sequential(\n",
    "            nn.Linear(d_v, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Print input shapes for debugging\n",
    "        # print(\"CrossAssetAttention Input x shape:\", x.shape)\n",
    "        # print(\"CrossAssetAttention Input mask shape:\", mask.shape)\n",
    "\n",
    "        # Ensure x is 3D (batch, assets, d_model)\n",
    "        if x.dim() == 4:\n",
    "            x = x.squeeze(0)  # Remove extra batch dimension if present\n",
    "        \n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D input, got {x.dim()}D tensor. Shape: {x.shape}\")\n",
    "\n",
    "        # Compute query, key, value\n",
    "        Q = self.WQ(x)  # (batch, assets, d_k)\n",
    "        K = self.WK(x)  # (batch, assets, d_k)\n",
    "        V = self.WV(x)  # (batch, assets, d_v)\n",
    "        \n",
    "        # Attention computation\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (batch, assets, assets)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Ensure mask is 2D\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.squeeze(0)\n",
    "            \n",
    "            if mask.dim() != 2:\n",
    "                raise ValueError(f\"Mask must be 2D, got {mask.dim()}D tensor\")\n",
    "            \n",
    "            attn = attn.masked_fill(~mask.unsqueeze(1), float('-inf'))\n",
    "        \n",
    "        # Softmax and value aggregation\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn_out = torch.matmul(attn, V)  # (batch, assets, d_v)\n",
    "        \n",
    "        # Score computation\n",
    "        scores = self.score_layer(attn_out).squeeze(-1)  # (batch, assets)\n",
    "        \n",
    "        # print(\"CrossAssetAttention Output scores shape:\", scores.shape)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "class PortfolioGenerator(nn.Module):\n",
    "    def __init__(self, G=5):\n",
    "        super().__init__()\n",
    "        self.G = G\n",
    "        self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def forward(self, scores, mask):\n",
    "        # print(\"PortfolioGenerator Input scores shape:\", scores.shape)\n",
    "        # print(\"PortfolioGenerator Input mask shape:\", mask.shape)\n",
    "\n",
    "        # Ensure scores is 2D\n",
    "        if scores.dim() > 2:\n",
    "            scores = scores.squeeze()\n",
    "        \n",
    "        if scores.dim() != 2:\n",
    "            raise ValueError(f\"Scores must be 2D, got {scores.dim()}D tensor. Shape: {scores.shape}\")\n",
    "        \n",
    "        # Ensure mask is 2D\n",
    "        if mask.dim() > 2:\n",
    "            mask = mask.squeeze()\n",
    "        \n",
    "        if mask.dim() != 2:\n",
    "            raise ValueError(f\"Mask must be 2D, got {mask.dim()}D tensor\")\n",
    "        \n",
    "        # Validate shapes\n",
    "        batch_size, num_assets = scores.shape\n",
    "        weights = torch.zeros_like(scores)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # More robust validity check\n",
    "            valid_assets = mask[i].sum().item()\n",
    "            \n",
    "            if valid_assets < 2*self.G:\n",
    "                print(f\"Warning: Batch {i} has insufficient valid assets. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Identify top and bottom G assets\n",
    "            top_indices = torch.topk(scores[i], min(self.G, valid_assets)).indices\n",
    "            bottom_indices = torch.topk(-scores[i], min(self.G, valid_assets)).indices\n",
    "            \n",
    "            # Compute weights\n",
    "            top_scores = scores[i, top_indices]\n",
    "            bottom_scores = scores[i, bottom_indices]\n",
    "            \n",
    "            top_weights = F.softmax(top_scores / self.temperature.clamp(min=0.1), dim=-1)\n",
    "            bottom_weights = F.softmax(-bottom_scores / self.temperature.clamp(min=0.1), dim=-1)\n",
    "            \n",
    "            weights[i, top_indices] = top_weights\n",
    "            weights[i, bottom_indices] = -bottom_weights\n",
    "        \n",
    "        # print(\"PortfolioGenerator Output weights shape:\", weights.shape)\n",
    "        return weights.nan_to_num(0.0)\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, transformer, attention, portfolio_gen, d_model=64):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        self.attention = attention\n",
    "        self.portfolio_gen = portfolio_gen\n",
    "        self.log_std = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Print input shapes for debugging\n",
    "        # print(\"PolicyNetwork Input x shape:\", x.shape)\n",
    "        # print(\"PolicyNetwork Input mask shape:\", mask.shape)\n",
    "\n",
    "        # Ensure correct input dimensions\n",
    "        if x.dim() == 5:  # Extra batch dimension\n",
    "            x = x.squeeze(0)\n",
    "        \n",
    "        # if x.dim() != 4:\n",
    "            # raise ValueError(f\"Expected 4D input tensor, got {x.dim()}D. Shape: {x.shape}\")\n",
    "        \n",
    "        # Process through transformer\n",
    "        x = self.transformer(x)  # (batch, assets, d_model)\n",
    "        \n",
    "        # Ensure mask is consistent\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.squeeze(0)\n",
    "        \n",
    "        # if mask.dim() != 2:\n",
    "            # raise ValueError(f\"Expected 2D mask, got {mask.dim()}D. Shape: {mask.shape}\")\n",
    "        \n",
    "        # Cross-asset attention scoring\n",
    "        scores = self.attention(x, mask)  # (batch, assets)\n",
    "        \n",
    "        # Portfolio generation\n",
    "        weights = self.portfolio_gen(scores, mask)  # (batch, assets)\n",
    "        \n",
    "        # Policy distribution\n",
    "        std = torch.exp(self.log_std).expand_as(weights)\n",
    "        dist = torch.distributions.Normal(weights, std)\n",
    "        \n",
    "        # Value estimation\n",
    "        values = self.value_head(x.mean(dim=1))\n",
    "        \n",
    "        return dist, values.squeeze(-1)\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, sequences, future_returns, masks, trainer, G=5):\n",
    "        super().__init__()\n",
    "        self.sequences = sequences\n",
    "        self.future_returns = future_returns\n",
    "        self.masks = masks\n",
    "        self.G = G\n",
    "        self.trainer = trainer  # Add this line\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=sequences[0].shape)\n",
    "        self.action_space = gym.spaces.Box(-1, 1, shape=(sequences.shape[1],))\n",
    "\n",
    "    def step(self, action):\n",
    "    # Get a random batch index\n",
    "        idx = np.random.randint(len(self.sequences))\n",
    "        \n",
    "        # Get future returns and mask\n",
    "        future_returns = self.future_returns[idx]\n",
    "        mask = self.masks[idx]\n",
    "        \n",
    "        # Ensure action is a tensor\n",
    "        action = torch.as_tensor(action, dtype=torch.float32)\n",
    "        \n",
    "        # Filter valid assets\n",
    "        valid = mask & (action != 0)\n",
    "        if valid.sum().item() < 2*self.G:\n",
    "            return None, -1.0, True, {}\n",
    "        \n",
    "        # Selected weights and returns\n",
    "        selected_weights = action[valid]\n",
    "        selected_returns = future_returns[valid]\n",
    "        \n",
    "        # Portfolio return calculation\n",
    "        portfolio_returns = (selected_weights.unsqueeze(-1) * selected_returns)\n",
    "        portfolio_cumulative_return = portfolio_returns.sum(dim=0)\n",
    "        \n",
    "        # More robust Sharpe Ratio calculation\n",
    "        mean_return = portfolio_cumulative_return.mean()\n",
    "        std_return = portfolio_cumulative_return.std()\n",
    "        sharpe_ratio = mean_return / (std_return + 1e-8)\n",
    "        \n",
    "        return None, sharpe_ratio.item(), True, {}\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(self, policy, env, lr=1e-4, gamma=0.99, clip=0.2):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip = clip\n",
    "    \n",
    "    def collect_trajectory(self):\n",
    "        states, masks, actions, log_probs, rewards = [], [], [], [], []\n",
    "        \n",
    "        # Determine the batch size dynamically\n",
    "        batch_size = len(self.env.sequences)\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            idx = np.random.randint(len(self.env.sequences))\n",
    "            state = self.env.sequences[idx]\n",
    "            mask = self.env.masks[idx]\n",
    "            \n",
    "            # Ensure consistent dimensions\n",
    "            if state.dim() == 3:\n",
    "                state = state.unsqueeze(0)  # Add batch dimension\n",
    "            elif state.dim() == 4:\n",
    "                state = state.squeeze(0)  # Remove extra dimension if present\n",
    "            \n",
    "            if mask.dim() == 1:\n",
    "                mask = mask.unsqueeze(0)  # Add batch dimension\n",
    "            elif mask.dim() == 2:\n",
    "                mask = mask.squeeze(0)  # Remove extra dimension if present\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    # Ensure state and mask have consistent batch dimension\n",
    "                    dist, values = self.policy(state, mask)\n",
    "                    action = dist.sample()\n",
    "                    log_prob = dist.log_prob(action)\n",
    "                    \n",
    "                    _, reward, _, _ = self.env.step(action.squeeze())\n",
    "                    \n",
    "                    states.append(state.squeeze(0))\n",
    "                    masks.append(mask.squeeze(0))\n",
    "                    actions.append(action.squeeze(0))\n",
    "                    log_probs.append(log_prob.squeeze(0))\n",
    "                    rewards.append(reward)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing trajectory: {e}\")\n",
    "                    # print(\"State shape:\", state.shape)\n",
    "                    # print(\"Mask shape:\", mask.shape)\n",
    "                    raise\n",
    "        \n",
    "        # Ensure all tensors have consistent shapes\n",
    "        states = torch.stack(states)\n",
    "        masks = torch.stack(masks)\n",
    "        actions = torch.stack(actions)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        \n",
    "        return states, masks, actions, log_probs, rewards\n",
    "\n",
    "    def train_epoch(self):\n",
    "        states, masks, actions, old_log_probs, rewards = self.collect_trajectory()\n",
    "        \n",
    "        # Ensure rewards have the correct shape for broadcasting\n",
    "        rewards = rewards.view(-1, 1)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
    "        \n",
    "        for _ in range(3):  # Multiple PPO update steps\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Get new policy outputs\n",
    "            dist, values = self.policy(states, masks)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Ensure consistent shapes\n",
    "            values = values.view(-1, 1)\n",
    "            \n",
    "            # Policy loss computation\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            surr1 = ratio * rewards\n",
    "            surr2 = torch.clamp(ratio, 1-self.clip, 1+self.clip) * rewards\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(values, rewards)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5*value_loss - 0.01*entropy\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def compute_average_sharpe(env, num_samples=100):\n",
    "    \"\"\"\n",
    "    Compute average Sharpe ratio across multiple random samples from the environment.\n",
    "    \n",
    "    Args:\n",
    "        env (PortfolioEnv): The portfolio environment\n",
    "        num_samples (int): Number of samples to compute Sharpe ratio\n",
    "\n",
    "    Returns:\n",
    "        float: Average Sharpe ratio\n",
    "    \"\"\"\n",
    "    sharpe_ratios = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select a time period\n",
    "        idx = np.random.randint(len(env.sequences))\n",
    "        \n",
    "        # Get state, mask, and future returns\n",
    "        state = env.sequences[idx]\n",
    "        mask = env.masks[idx]\n",
    "        future_returns = env.future_returns[idx]\n",
    "        \n",
    "        # Ensure state and mask are in correct dimensions\n",
    "        if state.dim() == 3:\n",
    "            state = state.unsqueeze(0)\n",
    "        if mask.dim() == 1:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        \n",
    "        try:\n",
    "            # Sample a portfolio weight using the policy in the environment's trainer\n",
    "            with torch.no_grad():\n",
    "                dist, _ = env.trainer.policy(state, mask)\n",
    "                action = dist.sample().squeeze()\n",
    "            \n",
    "            # Compute Sharpe ratio\n",
    "            valid = mask.squeeze() & (action != 0)\n",
    "            \n",
    "            if valid.sum().item() < 2*env.G:\n",
    "                continue\n",
    "            \n",
    "            selected_weights = action[valid]\n",
    "            selected_returns = future_returns[valid]\n",
    "            \n",
    "            # Portfolio return calculation\n",
    "            portfolio_returns = (selected_weights.unsqueeze(-1) * selected_returns)\n",
    "            portfolio_cumulative_return = portfolio_returns.sum(dim=0)\n",
    "            \n",
    "            # Sharpe Ratio calculation\n",
    "            mean_return = portfolio_cumulative_return.mean()\n",
    "            std_return = portfolio_cumulative_return.std()\n",
    "            sharpe_ratio = mean_return / (std_return + 1e-8)\n",
    "            \n",
    "            sharpe_ratios.append(sharpe_ratio.item())\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error computing Sharpe ratio: {e}\")\n",
    "    \n",
    "    # Return average Sharpe ratio\n",
    "    return np.mean(sharpe_ratios) if sharpe_ratios else -np.inf\n",
    "\n",
    "def plot_learning_curves(losses, sharpe_ratios, save_path='learning_curves.png'):\n",
    "    \"\"\"\n",
    "    Plot learning curves for losses and Sharpe ratios.\n",
    "    \n",
    "    Args:\n",
    "        losses (list): List of losses from training\n",
    "        sharpe_ratios (list): List of Sharpe ratios from evaluation\n",
    "        save_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses, label='Loss', color='blue')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Sharpe Ratio subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(sharpe_ratios, label='Sharpe Ratio', color='green')\n",
    "    plt.title('Portfolio Sharpe Ratio')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Sharpe Ratio')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "# Optional: Enhanced logging decorator\n",
    "def log_training_metrics(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Pre-training logging\n",
    "        print(\"Starting training epoch...\")\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Post-training logging (placeholder for custom metrics)\n",
    "        print(f\"Epoch completed. Additional metrics will be logged here.\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5213, grad_fn=<SubBackward0>)\n",
      "tensor(0.4946, grad_fn=<SubBackward0>)\n",
      "tensor(0.4841, grad_fn=<SubBackward0>)\n",
      "Epoch 1/30 | Loss: 0.4841 | Avg Sharpe: -0.0194\n",
      "tensor(0.4836, grad_fn=<SubBackward0>)\n",
      "tensor(0.4867, grad_fn=<SubBackward0>)\n",
      "tensor(0.4874, grad_fn=<SubBackward0>)\n",
      "Epoch 2/30 | Loss: 0.4874 | Avg Sharpe: -0.0115\n",
      "tensor(0.4877, grad_fn=<SubBackward0>)\n",
      "tensor(0.4851, grad_fn=<SubBackward0>)\n",
      "tensor(0.4835, grad_fn=<SubBackward0>)\n",
      "Epoch 3/30 | Loss: 0.4835 | Avg Sharpe: 0.0208\n",
      "tensor(0.4846, grad_fn=<SubBackward0>)\n",
      "tensor(0.4845, grad_fn=<SubBackward0>)\n",
      "tensor(0.4860, grad_fn=<SubBackward0>)\n",
      "Epoch 4/30 | Loss: 0.4860 | Avg Sharpe: -0.0394\n",
      "tensor(0.4838, grad_fn=<SubBackward0>)\n",
      "tensor(0.4823, grad_fn=<SubBackward0>)\n",
      "tensor(0.4814, grad_fn=<SubBackward0>)\n",
      "Epoch 5/30 | Loss: 0.4814 | Avg Sharpe: 0.0483\n",
      "tensor(0.4852, grad_fn=<SubBackward0>)\n",
      "tensor(0.4851, grad_fn=<SubBackward0>)\n",
      "tensor(0.4847, grad_fn=<SubBackward0>)\n",
      "Epoch 6/30 | Loss: 0.4847 | Avg Sharpe: 0.0282\n",
      "tensor(0.4834, grad_fn=<SubBackward0>)\n",
      "tensor(0.4850, grad_fn=<SubBackward0>)\n",
      "tensor(0.4850, grad_fn=<SubBackward0>)\n",
      "Epoch 7/30 | Loss: 0.4850 | Avg Sharpe: 0.0471\n",
      "tensor(0.4854, grad_fn=<SubBackward0>)\n",
      "tensor(0.4825, grad_fn=<SubBackward0>)\n",
      "tensor(0.4802, grad_fn=<SubBackward0>)\n",
      "Epoch 8/30 | Loss: 0.4802 | Avg Sharpe: 0.0354\n",
      "tensor(0.4829, grad_fn=<SubBackward0>)\n",
      "tensor(0.4822, grad_fn=<SubBackward0>)\n",
      "tensor(0.4820, grad_fn=<SubBackward0>)\n",
      "Epoch 9/30 | Loss: 0.4820 | Avg Sharpe: 0.0645\n",
      "tensor(0.4871, grad_fn=<SubBackward0>)\n",
      "tensor(0.4864, grad_fn=<SubBackward0>)\n",
      "tensor(0.4876, grad_fn=<SubBackward0>)\n",
      "Epoch 10/30 | Loss: 0.4876 | Avg Sharpe: -0.0110\n",
      "tensor(0.4853, grad_fn=<SubBackward0>)\n",
      "tensor(0.4836, grad_fn=<SubBackward0>)\n",
      "tensor(0.4839, grad_fn=<SubBackward0>)\n",
      "Epoch 11/30 | Loss: 0.4839 | Avg Sharpe: 0.0259\n",
      "tensor(0.4834, grad_fn=<SubBackward0>)\n",
      "tensor(0.4840, grad_fn=<SubBackward0>)\n",
      "tensor(0.4826, grad_fn=<SubBackward0>)\n",
      "Epoch 12/30 | Loss: 0.4826 | Avg Sharpe: 0.0690\n",
      "tensor(0.4824, grad_fn=<SubBackward0>)\n",
      "tensor(0.4826, grad_fn=<SubBackward0>)\n",
      "tensor(0.4829, grad_fn=<SubBackward0>)\n",
      "Epoch 13/30 | Loss: 0.4829 | Avg Sharpe: -0.0346\n",
      "tensor(0.4837, grad_fn=<SubBackward0>)\n",
      "tensor(0.4831, grad_fn=<SubBackward0>)\n",
      "tensor(0.4825, grad_fn=<SubBackward0>)\n",
      "Epoch 14/30 | Loss: 0.4825 | Avg Sharpe: 0.0160\n",
      "tensor(0.4861, grad_fn=<SubBackward0>)\n",
      "tensor(0.4859, grad_fn=<SubBackward0>)\n",
      "tensor(0.4849, grad_fn=<SubBackward0>)\n",
      "Epoch 15/30 | Loss: 0.4849 | Avg Sharpe: 0.0007\n",
      "tensor(0.4867, grad_fn=<SubBackward0>)\n",
      "tensor(0.4869, grad_fn=<SubBackward0>)\n",
      "tensor(0.4872, grad_fn=<SubBackward0>)\n",
      "Epoch 16/30 | Loss: 0.4872 | Avg Sharpe: 0.0644\n",
      "tensor(0.4828, grad_fn=<SubBackward0>)\n",
      "tensor(0.4829, grad_fn=<SubBackward0>)\n",
      "tensor(0.4827, grad_fn=<SubBackward0>)\n",
      "Epoch 17/30 | Loss: 0.4827 | Avg Sharpe: 0.0029\n",
      "tensor(0.4827, grad_fn=<SubBackward0>)\n",
      "tensor(0.4837, grad_fn=<SubBackward0>)\n",
      "tensor(0.4825, grad_fn=<SubBackward0>)\n",
      "Epoch 18/30 | Loss: 0.4825 | Avg Sharpe: -0.0735\n",
      "tensor(0.4829, grad_fn=<SubBackward0>)\n",
      "tensor(0.4829, grad_fn=<SubBackward0>)\n",
      "tensor(0.4839, grad_fn=<SubBackward0>)\n",
      "Epoch 19/30 | Loss: 0.4839 | Avg Sharpe: 0.0459\n",
      "tensor(0.4830, grad_fn=<SubBackward0>)\n",
      "tensor(0.4833, grad_fn=<SubBackward0>)\n",
      "tensor(0.4829, grad_fn=<SubBackward0>)\n",
      "Epoch 20/30 | Loss: 0.4829 | Avg Sharpe: -0.0216\n",
      "tensor(0.4823, grad_fn=<SubBackward0>)\n",
      "tensor(0.4828, grad_fn=<SubBackward0>)\n",
      "tensor(0.4829, grad_fn=<SubBackward0>)\n",
      "Epoch 21/30 | Loss: 0.4829 | Avg Sharpe: 0.0340\n",
      "tensor(0.4835, grad_fn=<SubBackward0>)\n",
      "tensor(0.4839, grad_fn=<SubBackward0>)\n",
      "tensor(0.4828, grad_fn=<SubBackward0>)\n",
      "Epoch 22/30 | Loss: 0.4828 | Avg Sharpe: -0.0404\n",
      "tensor(0.4760, grad_fn=<SubBackward0>)\n",
      "tensor(0.4749, grad_fn=<SubBackward0>)\n",
      "tensor(0.4726, grad_fn=<SubBackward0>)\n",
      "Epoch 23/30 | Loss: 0.4726 | Avg Sharpe: 0.0341\n",
      "tensor(0.4851, grad_fn=<SubBackward0>)\n",
      "tensor(0.4879, grad_fn=<SubBackward0>)\n",
      "tensor(0.4861, grad_fn=<SubBackward0>)\n",
      "Epoch 24/30 | Loss: 0.4861 | Avg Sharpe: 0.0655\n",
      "tensor(0.4841, grad_fn=<SubBackward0>)\n",
      "tensor(0.4825, grad_fn=<SubBackward0>)\n",
      "tensor(0.4823, grad_fn=<SubBackward0>)\n",
      "Epoch 25/30 | Loss: 0.4823 | Avg Sharpe: 0.0257\n",
      "tensor(0.4898, grad_fn=<SubBackward0>)\n",
      "tensor(0.4894, grad_fn=<SubBackward0>)\n",
      "tensor(0.4890, grad_fn=<SubBackward0>)\n",
      "Epoch 26/30 | Loss: 0.4890 | Avg Sharpe: 0.0003\n",
      "tensor(0.4832, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(trainer.optimizer, step_size=5, gamma=0.9)\n",
    "\n",
    "# Arrays to track metrics\n",
    "sharpe_ratios = []\n",
    "losses = []\n",
    "\n",
    "d_model = 128\n",
    "G = 4  # Top and bottom K stocks\n",
    "epochs = 30\n",
    "\n",
    "# Load your preprocessed data from AlphaPortfolioData\n",
    "\n",
    "\n",
    "# Network Components\n",
    "transformer = AssetTransformer(input_dim=features, d_model=d_model)\n",
    "attention = CrossAssetAttention(d_model=d_model)\n",
    "portfolio_gen = PortfolioGenerator(G=G)\n",
    "\n",
    "# Policy Network\n",
    "policy = PolicyNetwork(transformer, attention, portfolio_gen, d_model=d_model)\n",
    "\n",
    "# Portfolio Environment\n",
    "env = PortfolioEnv(\n",
    "    dataset.sequences,\n",
    "    dataset.future_returns,\n",
    "    dataset.masks,\n",
    "    trainer,  # Pass the trainer\n",
    "    G=G\n",
    ")\n",
    "\n",
    "# PPO Trainer\n",
    "trainer = PPOTrainer(policy, env)\n",
    "\n",
    "sharpe_ratios = []\n",
    "losses = []\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(trainer.optimizer, step_size=5, gamma=0.9)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train for an epoch\n",
    "    loss = trainer.train_epoch()\n",
    "    \n",
    "    # Compute average Sharpe ratio\n",
    "    avg_sharpe = compute_average_sharpe(env)\n",
    "    \n",
    "    # Track metrics\n",
    "    sharpe_ratios.append(avg_sharpe)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Learning rate decay\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss:.4f} | Avg Sharpe: {avg_sharpe:.4f}\")\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves(losses, sharpe_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
